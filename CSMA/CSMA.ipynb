{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qD7VAKNFYmsv"
   },
   "source": [
    "# :train and val .txt file creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wZrHMxNfYVhd",
    "outputId": "2eb98e35-1bda-4d3d-c997-04209aeb9880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the folder path containing the images and labels\n",
    "folder_path = '/workspace/Datasets/DIV2K_valid_HR'\n",
    "\n",
    "# Set the filename for the output text file\n",
    "output_file = '/workspace/val_files.txt'\n",
    "image_count = 0\n",
    "# Open the output file for writing\n",
    "with open(output_file, 'w') as f:\n",
    "    # Loop over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file is an image (e.g. .jpg or .png)\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            # Extract the label from the filename (assumes label is the first character(s) before the underscore)\n",
    "            label = filename.split('_')[0]\n",
    "            image_count += 1\n",
    "            # Write the label to the output file\n",
    "            f.write(label + '\\n')\n",
    "\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWHFXxNLxMB9"
   },
   "source": [
    "# datalabel end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rOEnvhSElwoe",
    "outputId": "a0baf144-1101-425f-e3a2-bc5d6fe5e682",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting ptflops\n",
      "  Downloading ptflops-0.7.3-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from ptflops) (1.13.0a0+936e930)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.8/dist-packages (from torch->ptflops) (4.4.0)\n",
      "Installing collected packages: ptflops\n",
      "Successfully installed ptflops-0.7.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ptflops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TYip1SOIlz5W",
    "outputId": "ad832a62-b50b-4cd3-c3b1-280e3119f468"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting pytorch-msssim\n",
      "  Downloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from pytorch-msssim) (1.13.0a0+936e930)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.8/dist-packages (from torch->pytorch-msssim) (4.4.0)\n",
      "Installing collected packages: pytorch-msssim\n",
      "Successfully installed pytorch-msssim-1.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch-msssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H66YBv8MZkvC",
    "outputId": "871a9f55-459b-41ee-a2af-125606c5bfc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting torchsummaryX\n",
      "  Downloading torchsummaryX-1.3.0-py3-none-any.whl (3.6 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchsummaryX) (1.22.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from torchsummaryX) (1.13.0a0+936e930)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from torchsummaryX) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->torchsummaryX) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->torchsummaryX) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->torchsummaryX) (1.16.0)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.8/dist-packages (from torch->torchsummaryX) (4.4.0)\n",
      "Installing collected packages: torchsummaryX\n",
      "Successfully installed torchsummaryX-1.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchsummaryX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.21.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.9 MB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx>=2.8\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 25.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting lazy_loader>=0.2\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Collecting tifffile>=2022.8.12\n",
      "  Downloading tifffile-2023.7.10-py3-none-any.whl (220 kB)\n",
      "\u001b[K     |████████████████████████████████| 220 kB 31.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting imageio>=2.27\n",
      "  Downloading imageio-2.35.1-py3-none-any.whl (315 kB)\n",
      "\u001b[K     |████████████████████████████████| 315 kB 43.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (1.22.2)\n",
      "Collecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.9 MB 17.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=21 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (21.3)\n",
      "Requirement already satisfied: pillow>=9.0.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (9.0.1)\n",
      "Collecting scipy>=1.8\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 34.5 MB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=21->scikit-image) (3.0.9)\n",
      "Installing collected packages: tifffile, scipy, PyWavelets, networkx, lazy-loader, imageio, scikit-image\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.6.3\n",
      "    Uninstalling scipy-1.6.3:\n",
      "      Successfully uninstalled scipy-1.6.3\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 2.6.3\n",
      "    Uninstalling networkx-2.6.3:\n",
      "      Successfully uninstalled networkx-2.6.3\n",
      "Successfully installed PyWavelets-1.4.1 imageio-2.35.1 lazy-loader-0.4 networkx-3.1 scikit-image-0.21.0 scipy-1.10.1 tifffile-2023.7.10\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCeF27AmgPih"
   },
   "source": [
    "Data management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "u2VIK90xgMXB"
   },
   "outputs": [],
   "source": [
    "# data_management.py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, Sampler\n",
    "from skimage.util import view_as_windows\n",
    "# from utils import load_image\n",
    "# from transforms import ToTensor\n",
    "\n",
    "\n",
    "def data_augmentation(image):\n",
    "    augmented_images_arrays, augmented_images_list = [], []\n",
    "    to_transform = [image, np.rot90(image, axes=(1, 2))]\n",
    "\n",
    "    for t in to_transform:\n",
    "        t_ud = t[:, ::-1, ...]\n",
    "        t_lr = t[:, :, ::-1, ...]\n",
    "        t_udlr = t_ud[:, :, ::-1, ...]\n",
    "\n",
    "        flips = [t_ud, t_lr, t_udlr]\n",
    "        augmented_images_arrays.extend(flips)\n",
    "\n",
    "    augmented_images_arrays.extend(to_transform)\n",
    "\n",
    "    for img in augmented_images_arrays:\n",
    "        img_unbatch = list(img)\n",
    "        augmented_images_list.extend(img_unbatch)\n",
    "\n",
    "    return augmented_images_list\n",
    "\n",
    "\n",
    "def create_patches(image, patch_size, step):\n",
    "    image = view_as_windows(image, patch_size, step)\n",
    "    h, w = image.shape[:2]\n",
    "    image = np.reshape(image, (h * w, patch_size[0], patch_size[1], patch_size[2]))\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "class DataSampler(Sampler):\n",
    "\n",
    "    def __init__(self, data_source, num_samples=None):\n",
    "        super().__init__(data_source)\n",
    "        self.data_source = data_source\n",
    "        self._num_samples = num_samples\n",
    "        self.rand = np.random.RandomState(0)\n",
    "        self.perm = []\n",
    "\n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        if self._num_samples is None:\n",
    "            return len(self.data_source)\n",
    "        return self._num_samples\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = len(self.data_source)\n",
    "        if self._num_samples is not None:\n",
    "            while len(self.perm) < self._num_samples:\n",
    "                perm = self.rand.permutation(n).astype('int32').tolist()\n",
    "                self.perm.extend(perm)\n",
    "            idx = self.perm[:self._num_samples]\n",
    "            self.perm = self.perm[self._num_samples:]\n",
    "        else:\n",
    "            idx = self.rand.permutation(n).astype('int32').tolist()\n",
    "\n",
    "        return iter(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "\n",
    "class NoisyImagesDataset(Dataset):\n",
    "    def __init__(self, files, channels, patch_size, transform=None, noise_transform=None):\n",
    "        self.channels = channels\n",
    "        self.patch_size = patch_size\n",
    "        self.transform = transform\n",
    "        self.noise_transforms = noise_transform\n",
    "        self.to_tensor = ToTensor()\n",
    "        self.dataset = {'image': [], 'noisy': []}\n",
    "        self.load_dataset(files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset['image'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, noisy = self.dataset.get('image')[idx], self.dataset.get('noisy')[idx]\n",
    "        sample = {'image': image, 'noisy': noisy}\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        sample = self.to_tensor(sample)\n",
    "\n",
    "        return sample.get('noisy'), sample.get('image')\n",
    "\n",
    "    def load_dataset(self, files):\n",
    "        patch_size = (self.patch_size, self.patch_size, self.channels)\n",
    "        for file in tqdm(files):\n",
    "            image = load_image(file, self.channels)\n",
    "            if image is None:\n",
    "                continue\n",
    "\n",
    "            image = create_patches(image, patch_size, step=self.patch_size)\n",
    "            sample = {'image': image, 'noisy': None}\n",
    "\n",
    "            for noise_transform in self.noise_transforms:\n",
    "                _sample = noise_transform(sample)\n",
    "                image, noisy = _sample['image'], _sample['noisy']\n",
    "                image, noisy = list(image), list(noisy)\n",
    "\n",
    "                self.dataset['image'].extend(image)\n",
    "                self.dataset['noisy'].extend(noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDlCF-s3gTtI"
   },
   "source": [
    "# Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8EAyV-3sgTTh"
   },
   "outputs": [],
   "source": [
    "# utils.py\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from skimage import io, color, img_as_ubyte\n",
    "\n",
    "\n",
    "def load_image(image_path, channels):\n",
    "    \"\"\"\n",
    "    Load image and change it color space from RGB to Grayscale if necessary.\n",
    "    :param image_path: str\n",
    "        Path of the image.\n",
    "    :param channels: int\n",
    "        Number of channels (3 for RGB, 1 for Grayscale)\n",
    "    :return: numpy array\n",
    "        Image loaded.\n",
    "    \"\"\"\n",
    "    image = io.imread(image_path)\n",
    "\n",
    "    if image.ndim == 3 and channels == 1:       # Convert from RGB to Grayscale and expand dims.\n",
    "        image = img_as_ubyte(color.rgb2gray(image))\n",
    "        return np.expand_dims(image, axis=-1)\n",
    "    elif image.ndim == 2 and channels == 1:     # Handling grayscale images if needed.\n",
    "        if image.dtype != 'uint8':\n",
    "            image = img_as_ubyte(image)\n",
    "        return np.expand_dims(image, axis=-1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def mod_crop(image, mod):\n",
    "    \"\"\"\n",
    "    Crops image according to mod to restore spatial dimensions\n",
    "    adequately in the decoding sections of the model.\n",
    "    :param image: numpy array\n",
    "        Image to crop.\n",
    "    :param mod: int\n",
    "        Module for padding allowed by the number of\n",
    "        encoding/decoding sections in the model.\n",
    "    :return: numpy array\n",
    "        Copped image\n",
    "    \"\"\"\n",
    "    size = image.shape[:2]\n",
    "    size = size - np.mod(size, mod)\n",
    "    image = image[:size[0], :size[1], ...]\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def mod_pad(image, mod):\n",
    "    \"\"\"\n",
    "    Pads image according to mod to restore spatial dimensions\n",
    "    adequately in the decoding sections of the model.\n",
    "    :param image: numpy array\n",
    "        Image to pad.\n",
    "    :param mod: int\n",
    "        Module for padding allowed by the number of\n",
    "        encoding/decoding sections in the model.\n",
    "    :return: numpy  array, tuple\n",
    "        Padded image, original image size.\n",
    "    \"\"\"\n",
    "    size = image.shape[:2]\n",
    "    h, w = np.mod(size, mod)\n",
    "    h, w = mod - h, mod - w\n",
    "    if h != mod or w != mod:\n",
    "        if image.ndim == 3:\n",
    "            image = np.pad(image, ((0, h), (0, w), (0, 0)), mode='reflect')\n",
    "        else:\n",
    "            image = np.pad(image, ((0, h), (0, w)), mode='reflect')\n",
    "\n",
    "    return image, size\n",
    "\n",
    "\n",
    "def set_seed(seed=1):\n",
    "    \"\"\"\n",
    "    Sets all random seeds.\n",
    "    :param seed: int\n",
    "        Seed value.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "def build_ensemble(image, normalize=True):\n",
    "    \"\"\"\n",
    "    Create image ensemble to estimate denoised image.\n",
    "    :param image: numpy array\n",
    "        Noisy image.\n",
    "    :param normalize: bool\n",
    "        Normalize image to range [0., 1.].\n",
    "    :return: list\n",
    "        Ensemble of noisy image transformed.\n",
    "    \"\"\"\n",
    "    img_rot = np.rot90(image)\n",
    "    ensemble_list = [\n",
    "        image, np.fliplr(image), np.flipud(image), np.flipud(np.fliplr(image)),\n",
    "        img_rot, np.fliplr(img_rot), np.flipud(img_rot), np.flipud(np.fliplr(img_rot))\n",
    "    ]\n",
    "\n",
    "    ensemble_transformed = []\n",
    "    for img in ensemble_list:\n",
    "        if img.ndim == 2:                                           # Expand dims for channel dimension in gray scale.\n",
    "            img = np.expand_dims(img.copy(), 0)                     # Use copy to avoid problems with reverse indexing.\n",
    "        else:\n",
    "            img = np.transpose(img.copy(), (2, 0, 1))               # Channels-first transposition.\n",
    "        if normalize:\n",
    "            img = img / 255.\n",
    "\n",
    "        img_t = torch.from_numpy(np.expand_dims(img, 0)).float()    # Expand dims again to create batch dimension.\n",
    "        ensemble_transformed.append(img_t)\n",
    "\n",
    "    return ensemble_transformed\n",
    "\n",
    "\n",
    "def separate_ensemble(ensemble, return_single=False):\n",
    "    \"\"\"\n",
    "    Apply inverse transforms to predicted image ensemble and average them.\n",
    "    :param ensemble: list\n",
    "        Predicted images, ensemble[0] is the original image,\n",
    "        and ensemble[i] is a transformed version of ensemble[i].\n",
    "    :param return_single: bool\n",
    "        Return also ensemble[0] to evaluate single prediction\n",
    "    :return: numpy array or tuple of numpy arrays\n",
    "        Average of the predicted images, original image denoised.\n",
    "    \"\"\"\n",
    "    ensemble_np = []\n",
    "\n",
    "    for img in ensemble:\n",
    "        img = img.squeeze()                     # Remove additional dimensions.\n",
    "        if img.ndim == 3:                       # Transpose if necessary.\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "\n",
    "        ensemble_np.append(img)\n",
    "\n",
    "    # Apply inverse transforms to vertical and horizontal flips.\n",
    "    img = ensemble_np[0] + np.fliplr(ensemble_np[1]) + np.flipud(ensemble_np[2]) + np.fliplr(np.flipud(ensemble_np[3]))\n",
    "\n",
    "    # Apply inverse transforms to 90º rotation, vertical and horizontal flips\n",
    "    img = img + np.rot90(ensemble_np[4], k=3) + np.rot90(np.fliplr(ensemble_np[5]), k=3)\n",
    "    img = img + np.rot90(np.flipud(ensemble_np[6]), k=3) + np.rot90(np.fliplr(np.flipud(ensemble_np[7])), k=3)\n",
    "\n",
    "    # Average and clip final predicted image.\n",
    "    img = img / 8.\n",
    "    img = np.clip(img, 0., 1.)\n",
    "\n",
    "    if return_single:\n",
    "        return img, ensemble_np[0]\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "\n",
    "def predict_ensemble(model, ensemble, device):\n",
    "    \"\"\"\n",
    "    Predict batch of images from an ensemble.\n",
    "    :param model: torch Module\n",
    "        Trained model to estimate denoised images.\n",
    "    :param ensemble: list\n",
    "        Images to estimate.\n",
    "    :param device: torch device\n",
    "        Device of the trained model.\n",
    "    :return: list\n",
    "        Estimated images of type numpy ndarray.\n",
    "    \"\"\"\n",
    "    y_hat_ensemble = []\n",
    "\n",
    "    for x in ensemble:\n",
    "        x = x.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_hat = model(x)\n",
    "            y_hat_ensemble.append(y_hat.cpu().detach().numpy().astype('float32'))\n",
    "\n",
    "    return y_hat_ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J27DJMXTgbcZ"
   },
   "source": [
    "Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3PWzbbVUgcFQ"
   },
   "outputs": [],
   "source": [
    "# transforms.py\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AdditiveWhiteGaussianNoise(object):\n",
    "    \"\"\"Additive white gaussian noise generator.\"\"\"\n",
    "    def __init__(self, noise_level, fix_sigma=False, clip=False):\n",
    "        self.noise_level = noise_level\n",
    "        self.fix_sigma = fix_sigma\n",
    "        self.rand = np.random.RandomState(1)\n",
    "        self.clip = clip\n",
    "        if not fix_sigma:\n",
    "            self.predefined_noise = [i for i in range(5, noise_level + 1, 5)]\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Generates additive white gaussian noise, and it is applied to the clean image.\n",
    "        :param sample:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        image = sample.get('image')\n",
    "\n",
    "        if image.ndim == 4:                 # if 'image' is a batch of images, we set a different noise level per image\n",
    "            samples = image.shape[0]        # (Samples, Height, Width, Channels) or (Samples, Channels, Height, Width)\n",
    "            if self.fix_sigma:\n",
    "                sigma = self.noise_level * np.ones((samples, 1, 1, 1))\n",
    "            else:\n",
    "                sigma = np.random.choice(self.predefined_noise, size=(samples, 1, 1, 1))\n",
    "            noise = self.rand.normal(0., 1., size=image.shape)\n",
    "            noise = noise * sigma\n",
    "        else:                               # else, 'image' is a simple image\n",
    "            if self.fix_sigma:              # (Height, Width, Channels) or (Channels , Height, Width)\n",
    "                sigma = self.noise_level\n",
    "            else:\n",
    "                sigma = self.rand.randint(5, self.noise_level)\n",
    "            noise = self.rand.normal(0., sigma, size=image.shape)\n",
    "\n",
    "        noisy = image + noise\n",
    "\n",
    "        if self.clip:\n",
    "            noisy = np.clip(noisy, 0., 255.)\n",
    "\n",
    "        return {'image': image, 'noisy': noisy.astype('float32')}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert data sample to pytorch tensor\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        image, noisy = sample.get('image'), sample.get('noisy')\n",
    "        image = torch.from_numpy(image.transpose((2, 0, 1)).astype('float32') / 255.)\n",
    "\n",
    "        if noisy is not None:\n",
    "            noisy = torch.from_numpy(noisy.transpose((2, 0, 1)).astype('float32') / 255.)\n",
    "\n",
    "        return {'image': image, 'noisy': noisy}\n",
    "\n",
    "\n",
    "class RandomVerticalFlip(object):\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if random.uniform(0., 1.) < self.p:\n",
    "            image, noisy = sample.get('image'), sample.get('noisy')\n",
    "            image = np.flipud(image)\n",
    "\n",
    "            if noisy is not None:\n",
    "                noisy = np.flipud(noisy)\n",
    "\n",
    "            return {'image': image, 'noisy': noisy}\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if random.uniform(0., 1.) < self.p:\n",
    "            image, noisy = sample.get('image'), sample.get('noisy')\n",
    "            image = np.fliplr(image)\n",
    "\n",
    "            if noisy is not None:\n",
    "                noisy = np.fliplr(noisy)\n",
    "\n",
    "            return {'image': image, 'noisy': noisy}\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class RandomRot90(object):\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if random.uniform(0., 1.) < self.p:\n",
    "            image, noisy = sample.get('image'), sample.get('noisy')\n",
    "            image = np.rot90(image)\n",
    "\n",
    "            if noisy is not None:\n",
    "                noisy = np.rot90(noisy)\n",
    "\n",
    "            return {'image': image, 'noisy': noisy}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJkMJ2uLhJJa"
   },
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "b3ta9us1hJgI"
   },
   "outputs": [],
   "source": [
    "# metrics.py\n",
    "import torch\n",
    "from pytorch_msssim import SSIM as _SSIM\n",
    "\n",
    "\n",
    "class PSNR(object):\n",
    "    r\"\"\"\n",
    "    Evaluates the PSNR metric in a tensor.\n",
    "    It can return a result with different reduction methods.\n",
    "\n",
    "    Args:\n",
    "        data_range (int, float): Range of the input images.\n",
    "        reduction (string): Specifies the reduction to apply to the output:\n",
    "            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
    "            ``'mean'``: the sum of the output will be divided by the number of\n",
    "            elements in the output, ``'sum'``: the output will be summed.\n",
    "        eps (float): Epsilon value to avoid division by zero.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_range, reduction='none', eps=1e-8):\n",
    "        self.data_range = data_range\n",
    "        self.reduction = reduction\n",
    "        self.eps = eps\n",
    "\n",
    "    def __call__(self, outputs, targets):\n",
    "        with torch.set_grad_enabled(False):\n",
    "            mse = torch.mean((outputs - targets) ** 2., dim=(1, 2, 3))\n",
    "            psnr = 10. * torch.log10((self.data_range ** 2.) / (mse + self.eps))\n",
    "\n",
    "            if self.reduction == 'mean':\n",
    "                return psnr.mean()\n",
    "            if self.reduction == 'sum':\n",
    "                return psnr.sum()\n",
    "\n",
    "            return psnr\n",
    "\n",
    "\n",
    "class SSIM(object):\n",
    "    r\"\"\"\n",
    "    Evaluates the SSIM metric in a tensor.\n",
    "    It can return a result with different reduction methods.\n",
    "\n",
    "    Args:\n",
    "        channels (int): Number of channels of the images.\n",
    "        data_range (int, float): Range of the input images.\n",
    "        reduction (string): Specifies the reduction to apply to the output:\n",
    "            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
    "            ``'mean'``: the sum of the output will be divided by the number of\n",
    "            elements in the output, ``'sum'``: the output will be summed.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, data_range, reduction='none'):\n",
    "        self.data_range = data_range\n",
    "        self.reduction = reduction\n",
    "        self.ssim_module = _SSIM(data_range=data_range, size_average=False, channel=channels)\n",
    "\n",
    "    def __call__(self, outputs, targets):\n",
    "        with torch.set_grad_enabled(False):\n",
    "            ssim = self.ssim_module(outputs, targets)\n",
    "\n",
    "            if self.reduction == 'mean':\n",
    "                return ssim.mean()\n",
    "            if self.reduction == 'sum':\n",
    "                return ssim.sum()\n",
    "\n",
    "            return ssim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcL46JsOhRMJ"
   },
   "source": [
    "# modified uNet + attn Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "wOqq0mqlbien"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "from typing import List, Tuple\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def init_weights(init_type='xavier'):\n",
    "    if init_type == 'xavier':\n",
    "        init = nn.init.xavier_normal_\n",
    "    elif init_type == 'he':\n",
    "        init = nn.init.kaiming_normal_\n",
    "    else:\n",
    "        init = nn.init.orthogonal_\n",
    "\n",
    "    def initializer(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv2d') != -1:\n",
    "            init(m.weight)\n",
    "        elif classname.find('BatchNorm') != -1:\n",
    "            nn.init.normal_(m.weight, 1.0, 0.01)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    return initializer\n",
    "\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.PReLU(out_channels)\n",
    "        self.stride = stride\n",
    "\n",
    "        # If the input and output channels differ, use a 1x1 convolution to match them\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class SkipPath(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SkipPath, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('skippath',x.shape)\n",
    "        x1 = self.conv1(x)\n",
    "        # print('skippath_con1',x1.shape)\n",
    "        x2 = self.conv2(x)\n",
    "        # print('skippath_con2',x2.shape)\n",
    "        x_0 = x1 + x2\n",
    "        # print('skippath_add',x_0.shape)\n",
    "\n",
    "        x3 = self.conv1(x_0)\n",
    "        x4 = self.conv2(x_0)\n",
    "        x_1 = x3 + x4\n",
    "\n",
    "        x5 = self.conv1(x_1)\n",
    "        x6 = self.conv2(x_1)\n",
    "        x_2 = x5 + x6\n",
    "\n",
    "        x7 = self.conv1(x_2)\n",
    "        x8 = self.conv2(x_2)\n",
    "        x = x7 + x8\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.res_block = ResBlock(out_channels, out_channels)\n",
    "        self.relu = nn.PReLU(out_channels)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('encoder',x.shape)\n",
    "        x = self.conv1(x)\n",
    "        # print('encoder_conv1',x.shape)\n",
    "        x = self.conv2(x)\n",
    "        # print('encoder_conv2',x.shape)\n",
    "        x = self.res_block(x)\n",
    "        # print('encoder_res',x.shape)\n",
    "        x = self.relu(x)\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "class Down_DC(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down_DC, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            EncoderBlock(in_channels, out_channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        # print('maxpool', x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.res_block = ResBlock(out_channels, out_channels)\n",
    "        self.relu = nn.PReLU(out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('decoder_in',x.shape)\n",
    "        x = self.conv1(x)\n",
    "        # print('dec_con1', x.shape)\n",
    "        x = self.conv2(x)\n",
    "        # print('dec_con2', x.shape)\n",
    "        x = self.res_block(x)\n",
    "        # print('dec_res', x.shape)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    # def forward(self, x, skip_connection):\n",
    "    #     print('Decoder_input',x.shape)\n",
    "    #     x = torch.cat([x, skip_connection], dim=1)\n",
    "    #     print('dec_cat',x.shape)\n",
    "    #     print('skip',skip_connection.shape)\n",
    "    #     x = self.conv1(x)\n",
    "    #     print('dec_con1', x.shape)\n",
    "    #     x = self.conv2(x)\n",
    "    #     print('dec_con2', x.shape)\n",
    "    #     x = self.res_block(x)\n",
    "    #     print('dec_res', x.shape)\n",
    "    #     x = self.relu(x)\n",
    "    #     return x\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpBlock, self).__init__()\n",
    "        self.Tconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('up_in',x.shape)\n",
    "        x = self.Tconv(x)\n",
    "        # print('up_out',x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class EnDecoderModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EnDecoderModule, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.res_block = ResBlock(out_channels, out_channels)\n",
    "        # self.Tconv = nn.ConvTranspose2d(out_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.relu = nn.PReLU(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('bottle',x.shape)\n",
    "        x = self.conv1(x)\n",
    "        # print('bottle',x.shape)\n",
    "        x = self.conv2(x)\n",
    "        x = self.res_block(x)\n",
    "        # print('bottle_res',x.shape)\n",
    "        x = self.relu(x)\n",
    "        # x = self.Tconv(x)\n",
    "        # print('bottle_tconv',x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class InputBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(InputBlock, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.conv_2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "\n",
    "        self.actv_1 = nn.PReLU(out_channels)\n",
    "        self.actv_2 = nn.PReLU(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.actv_1(self.conv_1(x))\n",
    "        return self.actv_2(self.conv_2(x))\n",
    "\n",
    "\n",
    "class OutputBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutputBlock, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels, in_channels, 3, padding=1)\n",
    "        self.conv_2 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "\n",
    "        self.actv_1 = nn.PReLU(in_channels)\n",
    "        self.actv_2 = nn.PReLU(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.actv_1(self.conv_1(x))\n",
    "        return self.actv_2(self.conv_2(x))\n",
    "\n",
    "class SequentialPolarizedSelfAttention(nn.Module):\n",
    "\n",
    "    # Multi-head Attention\n",
    "\n",
    "    def __init__(self, channel=512, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.ch_wv=nn.ModuleList([nn.Conv2d(channel, channel//2, kernel_size=(1,1)) for _ in range(num_heads)])\n",
    "        self.ch_wq=nn.ModuleList([nn.Conv2d(channel, 1, kernel_size=(1,1)) for _ in range(num_heads)])\n",
    "        self.softmax_channel=nn.Softmax(1)\n",
    "        self.softmax_spatial=nn.Softmax(-1)\n",
    "        self.ch_wz=nn.ModuleList([nn.Conv2d(channel//2, channel, kernel_size=(1,1)) for _ in range(num_heads)])\n",
    "        self.ln=nn.LayerNorm(channel)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        self.sp_wv=nn.ModuleList([nn.Conv2d(channel, channel//2, kernel_size=(1,1)) for _ in range(num_heads)])\n",
    "        self.sp_wq=nn.ModuleList([nn.Conv2d(channel, channel//2, kernel_size=(1,1)) for _ in range(num_heads)])\n",
    "        self.agp=nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.num_heads = num_heads\n",
    "        self.conv = nn.Conv2d(channel*num_heads, channel, kernel_size=(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "          b, c, h, w = x.size()\n",
    "          channel_out = 0\n",
    "          spatial_heads = []\n",
    "          for i in range(self.num_heads):\n",
    "              #Channel-only Self-Attention\n",
    "              channel_wv=self.ch_wv[i](x) #bs,c//2,h,w\n",
    "              channel_wq=self.ch_wq[i](x) #bs,1,h,w\n",
    "              channel_wv=channel_wv.reshape(b,c//2,-1) #bs,c//2,h*w\n",
    "              channel_wq=channel_wq.reshape(b,-1,1) #bs,h*w,1\n",
    "              channel_wq=self.softmax_channel(channel_wq)\n",
    "              channel_wz=torch.matmul(channel_wv,channel_wq).unsqueeze(-1) #bs,c//2,1,1\n",
    "              channel_weight=self.sigmoid(self.ch_wz[i](channel_wz).reshape(b,c,1).permute(0,2,1)).permute(0,2,1).reshape(b,c,1,1) #bs,c,1,1\n",
    "              channel_out=channel_weight*x\n",
    "\n",
    "              #Spatial-only Self-Attention\n",
    "              spatial_wv=self.sp_wv[i](channel_out) #bs,c//2,h,w\n",
    "              spatial_wq=self.sp_wq[i](channel_out) #bs,c//2,h,w\n",
    "              spatial_wq=self.agp(spatial_wq) #bs,c//2,1,1\n",
    "              spatial_wv=spatial_wv.reshape(b,c//2,-1) #bs,c//2,h*w\n",
    "              spatial_wq=spatial_wq.permute(0,2,3,1).reshape(b,1,c//2) #bs,1,c//2\n",
    "              spatial_wq=self.softmax_spatial(spatial_wq)\n",
    "              spatial_wz=torch.matmul(spatial_wq,spatial_wv) #bs,1,h*w\n",
    "              spatial_weight=self.sigmoid(spatial_wz.reshape(b,1,h,w)) #bs,1,h,w\n",
    "              spatial_out=spatial_weight*channel_out\n",
    "              spatial_heads.append(spatial_out)\n",
    "\n",
    "\n",
    "          # Combine results of Spatial-only Self-Attention across all heads\n",
    "          spatial_heads = torch.cat(spatial_heads, dim=1) #bs,num_heads,h,w\n",
    "          spatial_out = self.conv(spatial_heads) #bs,c,h,w\n",
    "          return spatial_out\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.relu = nn.PReLU(out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DeepRDU(nn.Module):\n",
    "    r\"\"\"\n",
    "    Residual-Dense U-net for image denoising.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        channels = kwargs['channels']\n",
    "        filters_0 = kwargs['base filters']\n",
    "        # filters_1 = 2 * filters_0\n",
    "        # filters_2 = 4 * filters_0\n",
    "        # filters_3 = 8 * filters_0\n",
    "        # filters_4 = 16 * filters_0\n",
    "\n",
    "        # Encoder:\n",
    "        # Level 0:\n",
    "        self.in_0 = InputBlock(channels, filters_0)\n",
    "\n",
    "        self.enc_0 = EncoderBlock(64, 64)\n",
    "#         self.enc_0 = EncoderBlock(1, 64)\n",
    "        self.DC_0 = Down_DC(64, 128)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Level 1:\n",
    "\n",
    "        self.enc_1 = EncoderBlock(128, 128)\n",
    "        self.DC_1 = Down_DC(128, 256)\n",
    "\n",
    "        # Level 2:\n",
    "        self.enc_2 = EncoderBlock(256, 256)\n",
    "        self.DC_2 = Down_DC(256, 512)\n",
    "\n",
    "        # Level 3:\n",
    "        self.enc_3 = EncoderBlock(512, 512)\n",
    "        self.DC_3 = Down_DC(512, 1024)\n",
    "\n",
    "\n",
    "        # Level 4 (Bottleneck)\n",
    "        self.EnDe = EnDecoderModule(1024, 1024)\n",
    "        self.EnDe_3 = UpBlock(1024, 1024)\n",
    "        # self.EnDe = EncoderBlock(1024, 1024)\n",
    "        # self.up_EnDe = UpBlock(1024, 1024)\n",
    "\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        # Level 3:\n",
    "\n",
    "        self.dec_3 = DecoderBlock(1024, 512)\n",
    "        # self.up_1_3 = UpBlock(1024, 1024)\n",
    "        self.up_3 = UpBlock(1024, 512)\n",
    "        self.sc_3 = SkipPath(512, 512)\n",
    "\n",
    "        # Level 2:\n",
    "\n",
    "        self.dec_2 = DecoderBlock(512, 256)\n",
    "        # self.up_1_3 = UpBlock(1024, 1024)\n",
    "        self.up_2 = UpBlock(512, 256)\n",
    "        self.sc_2 = SkipPath(256, 256)\n",
    "\n",
    "        # Level 1:\n",
    "\n",
    "\n",
    "        self.dec_1 = DecoderBlock(256, 128)\n",
    "        self.up_1 = UpBlock(256, 128)\n",
    "        self.sc_1 = SkipPath(128, 128)\n",
    "\n",
    "        # Level 0:\n",
    "\n",
    "\n",
    "        self.dec_0 = DecoderBlock(128, 64)\n",
    "        self.conv = nn.Conv2d(128,64, 3, padding=1)\n",
    "#         self.conv = nn.Conv2d(128,64, 1, padding=1)\n",
    "        self.sc_0 = SkipPath(64, 64)\n",
    "\n",
    "        self.at_con = ConvBlock(192, 128)\n",
    "        self.ca = SequentialPolarizedSelfAttention(128)\n",
    "\n",
    "\n",
    "\n",
    "        self.output_block = OutputBlock(128, 3)\n",
    "#         self.output_block = OutputBlock(128, 1)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "\n",
    "        out = self.in_0(inputs)\n",
    "#         print('input',inputs.shape)\n",
    "\n",
    "        out_0_1 = self.enc_0(out)\n",
    "#         out_0_1 = self.enc_0(inputs)\n",
    "        out_0 = self.DC_0(out_0_1)\n",
    "\n",
    "        out_1_1 = self.enc_1(out_0)\n",
    "        out_1 = self.DC_1(out_1_1)\n",
    "\n",
    "        out_2_1 = self.enc_2(out_1)\n",
    "        out_2 = self.DC_2(out_2_1)\n",
    "\n",
    "        out_3_1 = self.enc_3(out_2)\n",
    "        out_3 = self.DC_3(out_3_1)\n",
    "        #bottleneck\n",
    "        out_4 = self.EnDe(out_3)\n",
    "        out_4 = self.EnDe_3(out_4)\n",
    "\n",
    "        #Decoder\n",
    "        out_5 = self.dec_3(out_4)\n",
    "#         out_5_1 = self.sc_3(out_3_1)  #skip3\n",
    "#         out_5 = torch.cat([out_5, out_5_1], 1)\n",
    "        out_5 = torch.cat([out_5, out_3_1], 1)\n",
    "        out_6 = self.up_3(out_5)\n",
    "\n",
    "        out_7 = self.dec_2(out_6)\n",
    "\n",
    "        out_7_1 = self.sc_2(out_2_1)  #skip2\n",
    "        out_7 = torch.cat([out_7, out_7_1], 1)\n",
    "        # out_7 = torch.cat([out_7, out_2_1], 1)\n",
    "        out_8 = self.up_2(out_7)\n",
    "\n",
    "        out_8_1 = self.sc_1(out_1_1) #skip1\n",
    "        out_9 = self.dec_1(out_8)\n",
    "        out_9 = torch.cat([out_9, out_8_1], 1)\n",
    "\n",
    "        out_9 = self.up_1(out_9)\n",
    "\n",
    "        out_10_1 = self.sc_0(out_0_1) #skip0\n",
    "        out_10 = self.dec_0(out_9)\n",
    "        out_10 = torch.cat([out_10_1, out_10], 1)\n",
    "\n",
    "        #attention\n",
    "        out = torch.cat([out_10, out_0_1], 1)\n",
    "        out = self.at_con(out)\n",
    "        out = self.ca(out)\n",
    "\n",
    "        out = self.output_block(out)\n",
    "\n",
    "\n",
    "        out = inputs - out\n",
    "\n",
    "\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDhUR8Oxiico"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tbn6_xwFikIK"
   },
   "outputs": [],
   "source": [
    "# train.py\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "\n",
    "# from metrics import PSNR, SSIM\n",
    "\n",
    "\n",
    "\n",
    "class EpochLogger:\n",
    "    r\"\"\"\n",
    "    Keeps a log of metrics in the current epoch.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.log = {\n",
    "            'train loss': 0., 'train psnr': 0., 'train ssim': 0., 'val loss': 0., 'val psnr': 0., 'val ssim': 0.\n",
    "        }\n",
    "\n",
    "    def update_log(self, metrics, phase):\n",
    "        \"\"\"\n",
    "        Update the metrics in the current epoch, this method is called at every step of the epoch.\n",
    "        :param metrics: dict\n",
    "            Metrics to update: loss, PSNR and SSIM.\n",
    "        :param phase: str\n",
    "            Phase of the current epoch: training (train) or validation (val).\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        for key, value in metrics.items():\n",
    "            self.log[' '.join([phase, key])] += value\n",
    "\n",
    "    def get_log(self, n_samples, phase):\n",
    "        \"\"\"\n",
    "        Returns the average of the monitored metrics in the current moment,\n",
    "        given the number of evaluated samples.\n",
    "        :param n_samples: int\n",
    "            Number of evaluated samples.\n",
    "        :param phase: str\n",
    "            Phase of the current epoch: training (train) or validation (val).\n",
    "        :return: dic\n",
    "            Log of the current phase in the training.\n",
    "        \"\"\"\n",
    "        log = {\n",
    "            phase + ' loss': self.log[phase + ' loss'] / n_samples,\n",
    "            phase + ' psnr': self.log[phase + ' psnr'] / n_samples,\n",
    "            phase + ' ssim': self.log[phase + ' ssim'] / n_samples\n",
    "        }\n",
    "        return log\n",
    "\n",
    "\n",
    "class FileLogger(object):\n",
    "    \"\"\"\n",
    "    Keeps a log of the whole training and validation process.\n",
    "    The results are recorded in a CSV files.\n",
    "\n",
    "    Args:\n",
    "        file_path (string): path of the csv file.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"\n",
    "        Creates the csv record file.\n",
    "        :param f\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        header = ['epoch', 'lr', 'train loss', 'train psnr', 'train ssim', 'val loss', 'val psnr', 'val ssim']\n",
    "\n",
    "        with open(self.file_path, 'w') as csv_file:\n",
    "            file_writer = csv.writer(csv_file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            file_writer.writerow(header)\n",
    "\n",
    "    def __call__(self, epoch_log):\n",
    "        \"\"\"\n",
    "        Updates the CSV record file.\n",
    "        :param epoch_log: dict\n",
    "            Log of the current epoch.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        # Format log file:\n",
    "        # Epoch and learning rate:\n",
    "        log = ['{:03d}'.format(epoch_log['epoch']), '{:.5e}'.format(epoch_log['learning rate'])]\n",
    "\n",
    "        # Training loss, PSNR, SSIM:\n",
    "        log.extend([\n",
    "            '{:.5e}'.format(epoch_log['train loss']),\n",
    "            '{:.5f}'.format(epoch_log['train psnr']),\n",
    "            '{:.5f}'.format(epoch_log['train ssim'])\n",
    "        ])\n",
    "\n",
    "        # Validation loss, PSNR, SSIM\n",
    "        # Validation might not be done at all epochs, in that case the default calue is zero.\n",
    "        log.extend([\n",
    "            '{:.5e}'.format(epoch_log.get('val loss', 0.)),\n",
    "            '{:.5f}'.format(epoch_log.get('val psnr', 0.)),\n",
    "            '{:.5f}'.format(epoch_log.get('val ssim', 0.))\n",
    "        ])\n",
    "\n",
    "        with open(self.file_path, 'a') as csv_file:\n",
    "            file_writer = csv.writer(csv_file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            file_writer.writerow(log)\n",
    "\n",
    "\n",
    "def fit_model(model, data_loaders, channels, criterion, optimizer, scheduler, device, n_epochs, val_freq, checkpoint_dir, model_name):\n",
    "    \"\"\"\n",
    "    Training of the denoiser model.\n",
    "    :param model: torch Module\n",
    "        Neural network to fit.\n",
    "    :param data_loaders: dict\n",
    "        Dictionary with torch DataLoaders with training and validation datasets.\n",
    "    :param channels: int\n",
    "        Number of image channels\n",
    "    :param criterion: torch Module\n",
    "        Loss function.\n",
    "    :param optimizer: torch Optimizer\n",
    "        Gradient descent optimization algorithm.\n",
    "    :param scheduler: torch lr_scheduler\n",
    "        Learning rate scheduler.\n",
    "    :param device: torch device\n",
    "        Device used during training (CPU/GPU).\n",
    "    :param n_epochs: int\n",
    "        Number of epochs to fit the model.\n",
    "    :param val_freq: int\n",
    "        How many training epochs to run between validations.\n",
    "    :param checkpoint_dir: str\n",
    "        Path to the directory where the model checkpoints and CSV log files will be stored.\n",
    "    :param model_name: str\n",
    "        Prefix name of the trained model saved in checkpoint_dir.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    psnr = PSNR(data_range=1., reduction='sum')\n",
    "    ssim = SSIM(channels, data_range=1., reduction='sum')\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    logfile_path = os.path.join(checkpoint_dir,  ''.join([model_name, '_logfile.csv']))\n",
    "    model_path = os.path.join(checkpoint_dir, ''.join([model_name, '-{:03d}-{:.4e}-{:.4f}-{:.4f}.pth']))\n",
    "    file_logger = FileLogger(logfile_path)\n",
    "    best_model_path, best_psnr = '', -np.inf\n",
    "    since = time.time()\n",
    "\n",
    "    flag = True #False\n",
    "    CP = True\n",
    "\n",
    "\n",
    "    if flag == True :\n",
    "      checkpoint = torch.load('/workspace/Checkpoints/checkpoint.pt')\n",
    "      model.load_state_dict(checkpoint['model_state_dict'])\n",
    "      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "      epoch_start = checkpoint['epoch']\n",
    "      loss = checkpoint['loss']\n",
    "      step = checkpoint['step']\n",
    "      for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = checkpoint['lr']\n",
    "    else :\n",
    "      epoch_start = 0\n",
    "      step = 0\n",
    "      for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = optimizer.param_groups[0]['lr']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(epoch_start, n_epochs + 1):\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        epoch_logger = EpochLogger()\n",
    "        epoch_log = dict()\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                print('\\nEpoch: {}/{} - Learning rate: {:.4e}'.format(epoch, n_epochs, lr))\n",
    "                description = 'Training - Loss:{:.5e} - PSNR:{:.5f} - SSIM:{:.5f}'\n",
    "            elif phase == 'val' and epoch % val_freq == 0:\n",
    "                model.eval()\n",
    "                description = 'Validation - Loss:{:.5e} - PSNR:{:.5f} - SSIM:{:.5f}'\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            iterator = tqdm(enumerate(data_loaders[phase], 1), total=len(data_loaders[phase]), ncols=110)\n",
    "            iterator.set_description(description.format(0, 0, 0))\n",
    "            n_samples = 0\n",
    "\n",
    "            for step, (inputs, targets) in iterator:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                n_samples += inputs.size()[0]\n",
    "                metrics = {\n",
    "                    'loss': loss.item() * inputs.size()[0],\n",
    "                    'psnr': psnr(outputs, targets).item(),\n",
    "                    'ssim': ssim(outputs, targets).item()\n",
    "                }\n",
    "                epoch_logger.update_log(metrics, phase)\n",
    "                log = epoch_logger.get_log(n_samples, phase)\n",
    "                iterator.set_description(description.format(log[phase + ' loss'], log[phase + ' psnr'], log[phase + ' ssim']))\n",
    "\n",
    "            if phase == 'val':\n",
    "                # Apply Reduce LR On Plateau if it is the case and save the model if the validation PSNR is improved.\n",
    "                if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(log['val psnr'])\n",
    "                if log['val psnr'] > best_psnr:\n",
    "                    best_psnr = log['val psnr']\n",
    "\n",
    "                    # best_model_path = model_path.format(epoch, log['val loss'], log['val psnr'], log['val ssim'])\n",
    "                    best_model_path = model_path\n",
    "                    torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "            elif scheduler is not None:         # Apply another scheduler at epoch level.\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_log = {**epoch_log, **log}\n",
    "\n",
    "\n",
    "\n",
    "        # Save the current epoch metrics in a CVS file.\n",
    "        epoch_data = {'epoch': epoch, 'learning rate': lr, **epoch_log}\n",
    "        file_logger(epoch_data)\n",
    "\n",
    "        if CP == True :\n",
    "          torch.save({\n",
    "              'epoch': epoch + 1,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'loss': loss,\n",
    "              'step' : step,\n",
    "              'lr' : param_group[\"lr\"],\n",
    "              }, '/workspace/Checkpoints/checkpoint.pt')\n",
    "\n",
    "\n",
    "    # Save the last model and report training time.\n",
    "    # best_model_path = model_path.format(epoch, log['val loss'], log['val psnr'], log['val ssim'])\n",
    "    best_model_path = model_path\n",
    "    torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best PSNR: {:4f}'.format(best_psnr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTNVX2ZKiqFH"
   },
   "source": [
    "# Main train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb5xZJwtiqPG",
    "outputId": "0b059556-8c71-486c-d08a-bdfb6855a84f"
   },
   "outputs": [],
   "source": [
    "# import yaml\n",
    "# import torch\n",
    "# import torch.optim as optim\n",
    "# from os.path import join\n",
    "# from torch import nn\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision.transforms import transforms\n",
    "# from ptflops import get_model_complexity_info\n",
    "\n",
    "# from torchsummaryX import summary\n",
    "\n",
    "# # from model import RDUNet\n",
    "# # from data_management import NoisyImagesDataset, DataSampler\n",
    "# # from train import fit_model\n",
    "# # from transforms import AdditiveWhiteGaussianNoise, RandomHorizontalFlip, RandomVerticalFlip, RandomRot90\n",
    "# # from utils import set_seed\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     with open('/workspace/config.yaml', 'r') as stream:                # Load YAML configuration file.\n",
    "#         config = yaml.safe_load(stream)\n",
    "\n",
    "#     model_params = config['model']\n",
    "#     train_params = config['train']\n",
    "#     val_params = config['val']\n",
    "\n",
    "#     # Defining model:\n",
    "#     set_seed(0)\n",
    "#     drop_prob = 0.1\n",
    "#     model = DeepRDU(**model_params)\n",
    "#     # model = DN(**model_params)\n",
    "#     # model = RDUNet(**model_params)\n",
    "#     # model = UNet(n_classes = 1, depth = 4, padding = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print('Model summary:')\n",
    "#     test_shape = (model_params['channels'], train_params['patch size'], train_params['patch size'])\n",
    "#     with torch.no_grad():\n",
    "#         macs, params = get_model_complexity_info(model, test_shape, as_strings=True,\n",
    "#                                                  print_per_layer_stat=False, verbose=False)\n",
    "#         print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "#         print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "\n",
    "#     # Define the model name and use multi-GPU if it is allowed.\n",
    "#     model_name = 'model_color' if model_params['channels'] == 3 else 'model_gray'\n",
    "#     device = torch.device(train_params['device'])\n",
    "#     print(\"Using device: {}\".format(device))\n",
    "#     if torch.cuda.device_count() > 1 and 'cuda' in device.type and train_params['multi gpu']:\n",
    "#         model = nn.DataParallel(model)\n",
    "#         print('Using multiple GPUs')\n",
    "\n",
    "#     model = model.to(device)\n",
    "#     param_group = []\n",
    "#     for name, param in model.named_parameters():\n",
    "#         if 'conv' in name and 'weight' in name:\n",
    "#             p = {'params': param, 'weight_decay': train_params['weight decay']}\n",
    "#         else:\n",
    "#             p = {'params': param, 'weight_decay': 0.}\n",
    "#         param_group.append(p)\n",
    "\n",
    "#     # Load training and validation file names.\n",
    "#     # Modify .txt files if datasets do not fit in memory.\n",
    "#     with open('/workspace/train_files.txt', 'r') as f_train, open('/workspace/val_files.txt', 'r') as f_val:\n",
    "#         raw_train_files = f_train.read().splitlines()\n",
    "#         raw_val_files = f_val.read().splitlines()\n",
    "#         train_files = list(map(lambda file: join(train_params['dataset path'], file), raw_train_files))\n",
    "#         val_files = list(map(lambda file: join(val_params['dataset path'], file), raw_val_files))\n",
    "\n",
    "#     training_transforms = transforms.Compose([\n",
    "#         RandomHorizontalFlip(),\n",
    "#         RandomVerticalFlip(),\n",
    "#         RandomRot90()\n",
    "#     ])\n",
    "\n",
    "#     # Predefined noise level\n",
    "#     train_noise_transform = [AdditiveWhiteGaussianNoise(train_params['noise level'], clip=True)]\n",
    "#     val_noise_transforms = [AdditiveWhiteGaussianNoise(s, fix_sigma=True, clip=True) for s in val_params['noise levels']]\n",
    "\n",
    "#     print('\\nLoading training dataset:')\n",
    "#     training_dataset = NoisyImagesDataset(train_files,\n",
    "#                                           model_params['channels'],\n",
    "#                                           train_params['patch size'],\n",
    "#                                           training_transforms,\n",
    "#                                           train_noise_transform)\n",
    "\n",
    "#     print('\\nLoading validation dataset:')\n",
    "#     validation_dataset = NoisyImagesDataset(val_files,\n",
    "#                                             model_params['channels'],\n",
    "#                                             val_params['patch size'],\n",
    "#                                             None,\n",
    "#                                             val_noise_transforms)\n",
    "#     # Training in sub-epochs:\n",
    "#     print('Training patches:', len(training_dataset))\n",
    "#     print('Validation patches:', len(validation_dataset))\n",
    "#     n_samples = len(training_dataset) // train_params['dataset splits']\n",
    "#     n_epochs = train_params['epochs'] * train_params['dataset splits']\n",
    "#     sampler = DataSampler(training_dataset, num_samples=n_samples)\n",
    "\n",
    "#     data_loaders = {\n",
    "#         'train': DataLoader(training_dataset, train_params['batch size'], num_workers=train_params['workers'], sampler=sampler),\n",
    "#         'val': DataLoader(validation_dataset, val_params['batch size'], num_workers=val_params['workers']),\n",
    "#     }\n",
    "\n",
    "#     # Optimization:\n",
    "#     learning_rate = train_params['learning rate']\n",
    "#     step_size = train_params['scheduler step'] * train_params['dataset splits']\n",
    "\n",
    "#     criterion = nn.L1Loss()\n",
    "#     optimizer = optim.AdamW(param_group, lr=learning_rate)\n",
    "#     lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=train_params['scheduler gamma'])\n",
    "\n",
    "#     # Train the model\n",
    "#     fit_model(model, data_loaders, model_params['channels'], criterion, optimizer, lr_scheduler, device,\n",
    "#               n_epochs, val_params['frequency'], train_params['checkpoint path'], model_name)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n",
    "#     # \"\"## Model Compilation\"\"\"\n",
    "#     # base_filters = 128\n",
    "#     # channels = 3\n",
    "\n",
    "#     # model = RDUNet(base_filters, channels)\n",
    "#     # model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary:\n",
      "Computational complexity:       16.86 GMac\n",
      "Number of parameters:           121.27 M\n",
      "Model size:                     462.76 MB\n",
      "Inference latency:              9.01 ms\n",
      "Using multiple GPUs\n",
      "\n",
      "Loading training dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 800/800 [04:27<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading validation dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:12<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training patches: 522939\n",
      "Validation patches: 10794\n",
      "\n",
      "Epoch: 105/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.64323e-02 - PSNR:35.11794 - SSIM:0.90500: 100%|█████████| 3269/3269 [11:20<00:00,  4.80it/s]\n",
      "Validation - Loss:1.92763e-02 - PSNR:33.25536 - SSIM:0.86532: 100%|███████| 1350/1350 [02:54<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 106/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.63106e-02 - PSNR:35.18881 - SSIM:0.90612: 100%|█████████| 3269/3269 [11:05<00:00,  4.91it/s]\n",
      "Validation - Loss:1.84198e-02 - PSNR:33.49335 - SSIM:0.88001: 100%|███████| 1350/1350 [03:12<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 107/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.64443e-02 - PSNR:35.11454 - SSIM:0.90441: 100%|█████████| 3269/3269 [11:06<00:00,  4.91it/s]\n",
      "Validation - Loss:1.88700e-02 - PSNR:33.35095 - SSIM:0.87136: 100%|███████| 1350/1350 [03:11<00:00,  7.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 108/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.63591e-02 - PSNR:35.17065 - SSIM:0.90595: 100%|█████████| 3269/3269 [11:21<00:00,  4.80it/s]\n",
      "Validation - Loss:1.91358e-02 - PSNR:33.17848 - SSIM:0.86720: 100%|███████| 1350/1350 [03:12<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 109/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.63290e-02 - PSNR:35.19394 - SSIM:0.90590: 100%|█████████| 3269/3269 [11:11<00:00,  4.87it/s]\n",
      "Validation - Loss:1.95145e-02 - PSNR:33.15788 - SSIM:0.86058: 100%|███████| 1350/1350 [03:10<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 110/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.64073e-02 - PSNR:35.12811 - SSIM:0.90526: 100%|█████████| 3269/3269 [11:19<00:00,  4.81it/s]\n",
      "Validation - Loss:1.88699e-02 - PSNR:33.29340 - SSIM:0.87436: 100%|███████| 1350/1350 [02:52<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 111/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.63642e-02 - PSNR:35.15699 - SSIM:0.90546: 100%|█████████| 3269/3269 [11:19<00:00,  4.81it/s]\n",
      "Validation - Loss:2.06122e-02 - PSNR:32.83115 - SSIM:0.84199: 100%|███████| 1350/1350 [02:51<00:00,  7.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 112/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.63093e-02 - PSNR:35.20084 - SSIM:0.90614: 100%|█████████| 3269/3269 [11:09<00:00,  4.89it/s]\n",
      "Validation - Loss:1.85489e-02 - PSNR:33.44911 - SSIM:0.87825: 100%|███████| 1350/1350 [02:48<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 113/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.63361e-02 - PSNR:35.18783 - SSIM:0.90581: 100%|█████████| 3269/3269 [10:45<00:00,  5.07it/s]\n",
      "Validation - Loss:1.85527e-02 - PSNR:33.39878 - SSIM:0.87688: 100%|███████| 1350/1350 [02:49<00:00,  7.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 114/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.63186e-02 - PSNR:35.19197 - SSIM:0.90621: 100%|█████████| 3269/3269 [10:39<00:00,  5.11it/s]\n",
      "Validation - Loss:1.85150e-02 - PSNR:33.40105 - SSIM:0.87999: 100%|███████| 1350/1350 [02:48<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 115/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.63834e-02 - PSNR:35.16469 - SSIM:0.90563: 100%|█████████| 3269/3269 [11:13<00:00,  4.85it/s]\n",
      "Validation - Loss:1.75269e-02 - PSNR:33.83686 - SSIM:0.89152: 100%|███████| 1350/1350 [02:50<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 116/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.64168e-02 - PSNR:35.13228 - SSIM:0.90486: 100%|█████████| 3269/3269 [10:46<00:00,  5.06it/s]\n",
      "Validation - Loss:1.79646e-02 - PSNR:33.62786 - SSIM:0.88620: 100%|███████| 1350/1350 [02:51<00:00,  7.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 117/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.63121e-02 - PSNR:35.19237 - SSIM:0.90578: 100%|█████████| 3269/3269 [11:04<00:00,  4.92it/s]\n",
      "Validation - Loss:1.76802e-02 - PSNR:33.84379 - SSIM:0.89099: 100%|███████| 1350/1350 [03:07<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 118/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.62869e-02 - PSNR:35.19721 - SSIM:0.90601: 100%|█████████| 3269/3269 [11:15<00:00,  4.84it/s]\n",
      "Validation - Loss:1.88649e-02 - PSNR:33.34127 - SSIM:0.87036: 100%|███████| 1350/1350 [02:48<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 119/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.63953e-02 - PSNR:35.14587 - SSIM:0.90565: 100%|█████████| 3269/3269 [10:49<00:00,  5.03it/s]\n",
      "Validation - Loss:1.89603e-02 - PSNR:33.22877 - SSIM:0.87027: 100%|███████| 1350/1350 [02:53<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 120/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.63979e-02 - PSNR:35.16142 - SSIM:0.90581: 100%|█████████| 3269/3269 [11:17<00:00,  4.82it/s]\n",
      "Validation - Loss:2.03742e-02 - PSNR:32.89293 - SSIM:0.84772: 100%|███████| 1350/1350 [02:49<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 121/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.63171e-02 - PSNR:35.17976 - SSIM:0.90629: 100%|█████████| 3269/3269 [11:00<00:00,  4.95it/s]\n",
      "Validation - Loss:1.94794e-02 - PSNR:33.14035 - SSIM:0.86376: 100%|███████| 1350/1350 [02:48<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 122/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.63356e-02 - PSNR:35.18038 - SSIM:0.90578: 100%|█████████| 3269/3269 [11:10<00:00,  4.88it/s]\n",
      "Validation - Loss:1.91038e-02 - PSNR:33.23269 - SSIM:0.86744: 100%|███████| 1350/1350 [02:50<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 123/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.63133e-02 - PSNR:35.18588 - SSIM:0.90559: 100%|█████████| 3269/3269 [10:49<00:00,  5.03it/s]\n",
      "Validation - Loss:1.77369e-02 - PSNR:33.80292 - SSIM:0.88700: 100%|███████| 1350/1350 [02:53<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 124/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.63151e-02 - PSNR:35.19642 - SSIM:0.90589: 100%|█████████| 3269/3269 [11:07<00:00,  4.90it/s]\n",
      "Validation - Loss:1.96216e-02 - PSNR:33.05294 - SSIM:0.85801: 100%|███████| 1350/1350 [02:51<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 125/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.63115e-02 - PSNR:35.20604 - SSIM:0.90576: 100%|█████████| 3269/3269 [10:43<00:00,  5.08it/s]\n",
      "Validation - Loss:2.06355e-02 - PSNR:32.82948 - SSIM:0.84413: 100%|███████| 1350/1350 [02:47<00:00,  8.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 126/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.62955e-02 - PSNR:35.20983 - SSIM:0.90612: 100%|█████████| 3269/3269 [10:48<00:00,  5.04it/s]\n",
      "Validation - Loss:1.79805e-02 - PSNR:33.57939 - SSIM:0.88582: 100%|███████| 1350/1350 [02:51<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 127/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.63691e-02 - PSNR:35.15621 - SSIM:0.90539: 100%|█████████| 3269/3269 [11:33<00:00,  4.71it/s]\n",
      "Validation - Loss:1.79529e-02 - PSNR:33.68562 - SSIM:0.88637: 100%|███████| 1350/1350 [03:12<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 128/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.63116e-02 - PSNR:35.19708 - SSIM:0.90565: 100%|█████████| 3269/3269 [12:01<00:00,  4.53it/s]\n",
      "Validation - Loss:1.90426e-02 - PSNR:33.23120 - SSIM:0.86904: 100%|███████| 1350/1350 [03:09<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 129/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.62443e-02 - PSNR:35.23886 - SSIM:0.90652: 100%|█████████| 3269/3269 [11:25<00:00,  4.77it/s]\n",
      "Validation - Loss:2.19804e-02 - PSNR:32.41868 - SSIM:0.82258: 100%|███████| 1350/1350 [02:48<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 130/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.64276e-02 - PSNR:35.12796 - SSIM:0.90559: 100%|█████████| 3269/3269 [10:32<00:00,  5.16it/s]\n",
      "Validation - Loss:1.92873e-02 - PSNR:33.19252 - SSIM:0.86520: 100%|███████| 1350/1350 [02:47<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 131/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.63378e-02 - PSNR:35.15799 - SSIM:0.90567: 100%|█████████| 3269/3269 [11:05<00:00,  4.91it/s]\n",
      "Validation - Loss:1.93118e-02 - PSNR:33.07373 - SSIM:0.86456: 100%|███████| 1350/1350 [02:52<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 132/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.63570e-02 - PSNR:35.16311 - SSIM:0.90569: 100%|█████████| 3269/3269 [10:59<00:00,  4.96it/s]\n",
      "Validation - Loss:1.99440e-02 - PSNR:33.00771 - SSIM:0.85584: 100%|███████| 1350/1350 [02:56<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 133/210 - Learning rate: 2.5000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Loss:1.62951e-02 - PSNR:35.19949 - SSIM:0.90645:  31%|██▊      | 1006/3269 [03:51<08:15,  4.56it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import yaml\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from os.path import join\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "# from model import DeepRDU\n",
    "# from data_management import NoisyImagesDataset, DataSampler\n",
    "# from train import fit_model\n",
    "# from transforms import AdditiveWhiteGaussianNoise, RandomHorizontalFlip, RandomVerticalFlip, RandomRot90\n",
    "# from utils import set_seed\n",
    "\n",
    "\n",
    "def write_model_summary_to_csv(file_path, model_name, macs, params, model_size, latency_ms):\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "    with open(file_path, mode='a', newline='') as csv_file:\n",
    "        fieldnames = ['Model Name', 'MACs', 'Parameters', 'Size (MB)', 'Latency (ms)']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow({\n",
    "            'Model Name': model_name,\n",
    "            'MACs': macs,\n",
    "            'Parameters': params,\n",
    "            'Size (MB)': f\"{model_size:.2f}\",\n",
    "            'Latency (ms)': f\"{latency_ms:.2f}\"\n",
    "        })\n",
    "\n",
    "\n",
    "def main():\n",
    "    with open('/workspace/config.yaml', 'r') as stream:\n",
    "        config = yaml.safe_load(stream)\n",
    "\n",
    "    model_params = config['model']\n",
    "    train_params = config['train']\n",
    "    val_params = config['val']\n",
    "\n",
    "    set_seed(0)\n",
    "    model = DeepRDU(**model_params)\n",
    "\n",
    "    print('Model summary:')\n",
    "    test_shape = (model_params['channels'], train_params['patch size'], train_params['patch size'])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        macs, params = get_model_complexity_info(\n",
    "            model, test_shape, as_strings=True,\n",
    "            print_per_layer_stat=False, verbose=False\n",
    "        )\n",
    "\n",
    "        # Model size\n",
    "        torch.save(model.state_dict(), \"temp.pth\")\n",
    "        model_size_mb = os.path.getsize(\"temp.pth\") / (1024 * 1024)\n",
    "        os.remove(\"temp.pth\")\n",
    "\n",
    "        # Inference latency\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "        dummy_input = torch.randn(1, *test_shape).to(device)\n",
    "        start_time = time.time()\n",
    "        for _ in range(50):\n",
    "            _ = model(dummy_input)\n",
    "        end_time = time.time()\n",
    "        latency_ms = ((end_time - start_time) / 50) * 1000\n",
    "\n",
    "        print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "        print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "        print('{:<30}  {:.2f} MB'.format('Model size: ', model_size_mb))\n",
    "        print('{:<30}  {:.2f} ms'.format('Inference latency: ', latency_ms))\n",
    "\n",
    "        # Write to CSV\n",
    "        summary_file_path = '/workspace/model_summary.csv'\n",
    "        write_model_summary_to_csv(summary_file_path, 'DeepRDU', macs, params, model_size_mb, latency_ms)\n",
    "\n",
    "    model_name = 'model_color' if model_params['channels'] == 3 else 'model_gray'\n",
    "    if torch.cuda.device_count() > 1 and 'cuda' in device.type and train_params['multi gpu']:\n",
    "        model = nn.DataParallel(model)\n",
    "        print('Using multiple GPUs')\n",
    "\n",
    "    model = model.to(device)\n",
    "    param_group = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'conv' in name and 'weight' in name:\n",
    "            p = {'params': param, 'weight_decay': train_params['weight decay']}\n",
    "        else:\n",
    "            p = {'params': param, 'weight_decay': 0.}\n",
    "        param_group.append(p)\n",
    "\n",
    "    with open('/workspace/train_files.txt', 'r') as f_train, open('/workspace/val_files.txt', 'r') as f_val:\n",
    "        raw_train_files = f_train.read().splitlines()\n",
    "        raw_val_files = f_val.read().splitlines()\n",
    "        train_files = list(map(lambda file: join(train_params['dataset path'], file), raw_train_files))\n",
    "        val_files = list(map(lambda file: join(val_params['dataset path'], file), raw_val_files))\n",
    "\n",
    "    training_transforms = transforms.Compose([\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomVerticalFlip(),\n",
    "        RandomRot90()\n",
    "    ])\n",
    "\n",
    "    train_noise_transform = [AdditiveWhiteGaussianNoise(train_params['noise level'], clip=True)]\n",
    "    val_noise_transforms = [AdditiveWhiteGaussianNoise(s, fix_sigma=True, clip=True) for s in val_params['noise levels']]\n",
    "\n",
    "    print('\\nLoading training dataset:')\n",
    "    training_dataset = NoisyImagesDataset(train_files, model_params['channels'],\n",
    "                                          train_params['patch size'], training_transforms, train_noise_transform)\n",
    "\n",
    "    print('\\nLoading validation dataset:')\n",
    "    validation_dataset = NoisyImagesDataset(val_files, model_params['channels'],\n",
    "                                            val_params['patch size'], None, val_noise_transforms)\n",
    "\n",
    "    print('Training patches:', len(training_dataset))\n",
    "    print('Validation patches:', len(validation_dataset))\n",
    "    n_samples = len(training_dataset) // train_params['dataset splits']\n",
    "    n_epochs = train_params['epochs'] * train_params['dataset splits']\n",
    "    sampler = DataSampler(training_dataset, num_samples=n_samples)\n",
    "\n",
    "    data_loaders = {\n",
    "        'train': DataLoader(training_dataset, train_params['batch size'], num_workers=train_params['workers'], sampler=sampler),\n",
    "        'val': DataLoader(validation_dataset, val_params['batch size'], num_workers=val_params['workers']),\n",
    "    }\n",
    "\n",
    "    # Optimization:\n",
    "    learning_rate = train_params['learning rate']\n",
    "    step_size = train_params['scheduler step'] * train_params['dataset splits']\n",
    "\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.AdamW(param_group, lr=learning_rate)\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=train_params['scheduler gamma'])\n",
    "\n",
    "    # Train the model\n",
    "    fit_model(model, data_loaders, model_params['channels'], criterion, optimizer, lr_scheduler, device,\n",
    "              n_epochs, val_params['frequency'], train_params['checkpoint path'], model_name)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ji-QJ0LLAqw"
   },
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eq_TIQB3LAqx",
    "outputId": "2e433e40-2755-48b5-df4b-34cdcf606f03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Noisy Dataset:  noisy_bsd68_\n",
      "Gt Dataset: bsd68_label\n",
      "extension _gray\n",
      "noise level: 10\n",
      "base directory /content/drive/MyDrive/RDUNet-main/Datasets/Test_matfile\n",
      "Generated Noisy Path: /content/drive/MyDrive/RDUNet-main/Datasets/Test_matfile/noisy_bsd68_10_gray.mat\n",
      "Generated GT Path: /content/drive/MyDrive/RDUNet-main/Datasets/Test_matfile/bsd68_label_gray.mat\n",
      "68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-7c13dd7193a1>:61: FutureWarning: `multichannel` is a deprecated argument name for `structural_similarity`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  ssim = structural_similarity(y, y_hat, data_range=1., multichannel=multi_channel, gaussian_weights=True,\n",
      "<ipython-input-14-7c13dd7193a1>:65: FutureWarning: `multichannel` is a deprecated argument name for `structural_similarity`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  ssim_ens = structural_similarity(y, y_hat_ens, data_range=1., multichannel=multi_channel,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 1 - PSNR: 29.8978 - SSIM: 0.8552 - ens PSNR: 30.0117 - ens SSIM: 0.8587\n",
      "Image: 2 - PSNR: 34.2142 - SSIM: 0.9372 - ens PSNR: 34.3984 - ens SSIM: 0.9394\n",
      "Image: 3 - PSNR: 33.8786 - SSIM: 0.9141 - ens PSNR: 33.9831 - ens SSIM: 0.9153\n",
      "Image: 4 - PSNR: 35.0214 - SSIM: 0.9319 - ens PSNR: 35.1360 - ens SSIM: 0.9334\n",
      "Image: 5 - PSNR: 32.1384 - SSIM: 0.9263 - ens PSNR: 32.2327 - ens SSIM: 0.9281\n",
      "Image: 6 - PSNR: 36.6900 - SSIM: 0.9481 - ens PSNR: 36.8303 - ens SSIM: 0.9496\n",
      "Image: 7 - PSNR: 33.0203 - SSIM: 0.9203 - ens PSNR: 33.1457 - ens SSIM: 0.9220\n",
      "Image: 8 - PSNR: 31.0953 - SSIM: 0.9064 - ens PSNR: 31.1612 - ens SSIM: 0.9077\n",
      "Image: 9 - PSNR: 33.2749 - SSIM: 0.9252 - ens PSNR: 33.3835 - ens SSIM: 0.9270\n",
      "Image: 10 - PSNR: 33.8911 - SSIM: 0.9124 - ens PSNR: 34.0049 - ens SSIM: 0.9149\n",
      "Image: 11 - PSNR: 32.7461 - SSIM: 0.9339 - ens PSNR: 32.9808 - ens SSIM: 0.9368\n",
      "Image: 12 - PSNR: 32.6704 - SSIM: 0.8973 - ens PSNR: 32.7844 - ens SSIM: 0.8993\n",
      "Image: 13 - PSNR: 33.7737 - SSIM: 0.9178 - ens PSNR: 33.8507 - ens SSIM: 0.9192\n",
      "Image: 14 - PSNR: 34.6374 - SSIM: 0.9164 - ens PSNR: 34.7770 - ens SSIM: 0.9184\n",
      "Image: 15 - PSNR: 31.8207 - SSIM: 0.8971 - ens PSNR: 31.9510 - ens SSIM: 0.8989\n",
      "Image: 16 - PSNR: 32.4399 - SSIM: 0.9311 - ens PSNR: 32.5793 - ens SSIM: 0.9326\n",
      "Image: 17 - PSNR: 36.2748 - SSIM: 0.9253 - ens PSNR: 36.3664 - ens SSIM: 0.9272\n",
      "Image: 18 - PSNR: 36.6473 - SSIM: 0.9416 - ens PSNR: 36.7561 - ens SSIM: 0.9433\n",
      "Image: 19 - PSNR: 32.5050 - SSIM: 0.9090 - ens PSNR: 32.6057 - ens SSIM: 0.9108\n",
      "Image: 20 - PSNR: 33.2301 - SSIM: 0.9161 - ens PSNR: 33.3609 - ens SSIM: 0.9181\n",
      "Image: 21 - PSNR: 31.2590 - SSIM: 0.9404 - ens PSNR: 31.3508 - ens SSIM: 0.9419\n",
      "Image: 22 - PSNR: 31.4531 - SSIM: 0.9196 - ens PSNR: 31.5648 - ens SSIM: 0.9214\n",
      "Image: 23 - PSNR: 33.5661 - SSIM: 0.9367 - ens PSNR: 33.6917 - ens SSIM: 0.9385\n",
      "Image: 24 - PSNR: 33.0781 - SSIM: 0.9332 - ens PSNR: 33.1860 - ens SSIM: 0.9346\n",
      "Image: 25 - PSNR: 34.1546 - SSIM: 0.9635 - ens PSNR: 34.2586 - ens SSIM: 0.9647\n",
      "Image: 26 - PSNR: 32.5252 - SSIM: 0.8981 - ens PSNR: 32.6341 - ens SSIM: 0.8998\n",
      "Image: 27 - PSNR: 34.0063 - SSIM: 0.9087 - ens PSNR: 34.1209 - ens SSIM: 0.9107\n",
      "Image: 28 - PSNR: 36.2633 - SSIM: 0.9239 - ens PSNR: 36.4966 - ens SSIM: 0.9264\n",
      "Image: 29 - PSNR: 29.4998 - SSIM: 0.9426 - ens PSNR: 29.5907 - ens SSIM: 0.9445\n",
      "Image: 30 - PSNR: 34.0210 - SSIM: 0.9072 - ens PSNR: 34.1355 - ens SSIM: 0.9091\n",
      "Image: 31 - PSNR: 29.4829 - SSIM: 0.9349 - ens PSNR: 29.5314 - ens SSIM: 0.9355\n",
      "Image: 32 - PSNR: 31.4824 - SSIM: 0.9311 - ens PSNR: 31.5834 - ens SSIM: 0.9324\n",
      "Image: 33 - PSNR: 32.6455 - SSIM: 0.9545 - ens PSNR: 32.7239 - ens SSIM: 0.9554\n",
      "Image: 34 - PSNR: 36.1212 - SSIM: 0.8992 - ens PSNR: 36.2552 - ens SSIM: 0.9014\n",
      "Image: 35 - PSNR: 32.2838 - SSIM: 0.9091 - ens PSNR: 32.3717 - ens SSIM: 0.9108\n",
      "Image: 36 - PSNR: 30.6356 - SSIM: 0.7935 - ens PSNR: 30.7472 - ens SSIM: 0.7994\n",
      "Image: 37 - PSNR: 32.3519 - SSIM: 0.8970 - ens PSNR: 32.3979 - ens SSIM: 0.8979\n",
      "Image: 38 - PSNR: 33.3857 - SSIM: 0.9101 - ens PSNR: 33.4948 - ens SSIM: 0.9118\n",
      "Image: 39 - PSNR: 37.3565 - SSIM: 0.9560 - ens PSNR: 37.5429 - ens SSIM: 0.9575\n",
      "Image: 40 - PSNR: 32.7092 - SSIM: 0.8613 - ens PSNR: 32.8350 - ens SSIM: 0.8636\n",
      "Image: 41 - PSNR: 32.1327 - SSIM: 0.9190 - ens PSNR: 32.2410 - ens SSIM: 0.9206\n",
      "Image: 42 - PSNR: 33.5420 - SSIM: 0.9000 - ens PSNR: 33.6574 - ens SSIM: 0.9020\n",
      "Image: 43 - PSNR: 34.6404 - SSIM: 0.9446 - ens PSNR: 34.8366 - ens SSIM: 0.9466\n",
      "Image: 44 - PSNR: 31.1321 - SSIM: 0.9252 - ens PSNR: 31.2449 - ens SSIM: 0.9268\n",
      "Image: 45 - PSNR: 36.7735 - SSIM: 0.9232 - ens PSNR: 36.8894 - ens SSIM: 0.9250\n",
      "Image: 46 - PSNR: 30.8952 - SSIM: 0.8987 - ens PSNR: 31.0125 - ens SSIM: 0.9001\n",
      "Image: 47 - PSNR: 31.2631 - SSIM: 0.9123 - ens PSNR: 31.3586 - ens SSIM: 0.9145\n",
      "Image: 48 - PSNR: 34.3495 - SSIM: 0.9597 - ens PSNR: 34.5919 - ens SSIM: 0.9617\n",
      "Image: 49 - PSNR: 34.6019 - SSIM: 0.8819 - ens PSNR: 34.6863 - ens SSIM: 0.8837\n",
      "Image: 50 - PSNR: 31.3658 - SSIM: 0.9063 - ens PSNR: 31.4545 - ens SSIM: 0.9081\n",
      "Image: 51 - PSNR: 32.4566 - SSIM: 0.9164 - ens PSNR: 32.6396 - ens SSIM: 0.9180\n",
      "Image: 52 - PSNR: 35.2163 - SSIM: 0.9028 - ens PSNR: 35.2902 - ens SSIM: 0.9039\n",
      "Image: 53 - PSNR: 34.2614 - SSIM: 0.8658 - ens PSNR: 34.3352 - ens SSIM: 0.8680\n",
      "Image: 54 - PSNR: 33.2598 - SSIM: 0.9099 - ens PSNR: 33.3979 - ens SSIM: 0.9121\n",
      "Image: 55 - PSNR: 31.9596 - SSIM: 0.9049 - ens PSNR: 32.0866 - ens SSIM: 0.9070\n",
      "Image: 56 - PSNR: 29.3689 - SSIM: 0.9350 - ens PSNR: 29.4279 - ens SSIM: 0.9355\n",
      "Image: 57 - PSNR: 32.5913 - SSIM: 0.9246 - ens PSNR: 32.6636 - ens SSIM: 0.9259\n",
      "Image: 58 - PSNR: 33.9512 - SSIM: 0.8695 - ens PSNR: 33.9925 - ens SSIM: 0.8703\n",
      "Image: 59 - PSNR: 33.6053 - SSIM: 0.8722 - ens PSNR: 33.6852 - ens SSIM: 0.8739\n",
      "Image: 60 - PSNR: 34.2246 - SSIM: 0.8765 - ens PSNR: 34.2924 - ens SSIM: 0.8776\n",
      "Image: 61 - PSNR: 33.9181 - SSIM: 0.8859 - ens PSNR: 34.0265 - ens SSIM: 0.8878\n",
      "Image: 62 - PSNR: 37.2160 - SSIM: 0.9560 - ens PSNR: 37.5319 - ens SSIM: 0.9578\n",
      "Image: 63 - PSNR: 31.4323 - SSIM: 0.9339 - ens PSNR: 31.5484 - ens SSIM: 0.9355\n",
      "Image: 64 - PSNR: 30.8450 - SSIM: 0.8759 - ens PSNR: 30.8900 - ens SSIM: 0.8760\n",
      "Image: 65 - PSNR: 33.1525 - SSIM: 0.9154 - ens PSNR: 33.2434 - ens SSIM: 0.9170\n",
      "Image: 66 - PSNR: 41.5975 - SSIM: 0.9762 - ens PSNR: 41.8007 - ens SSIM: 0.9776\n",
      "Image: 67 - PSNR: 29.7877 - SSIM: 0.9458 - ens PSNR: 29.9143 - ens SSIM: 0.9466\n",
      "Image: 68 - PSNR: 31.2520 - SSIM: 0.9506 - ens PSNR: 31.3631 - ens SSIM: 0.9518\n",
      "sigma = 10 - PSNR: 33.2197 - SSIM: 0.9158 - ens PSNR: 33.3370 - ens SSIM: 0.9175 \n",
      "\n",
      "extension _gray\n",
      "noise level: 30\n",
      "base directory /content/drive/MyDrive/RDUNet-main/Datasets/Test_matfile\n",
      "Generated Noisy Path: /content/drive/MyDrive/RDUNet-main/Datasets/Test_matfile/noisy_bsd68_30_gray.mat\n",
      "Generated GT Path: /content/drive/MyDrive/RDUNet-main/Datasets/Test_matfile/bsd68_label_gray.mat\n",
      "68\n",
      "Image: 1 - PSNR: 24.0657 - SSIM: 0.5850 - ens PSNR: 24.0971 - ens SSIM: 0.5870\n",
      "Image: 2 - PSNR: 27.5793 - SSIM: 0.7722 - ens PSNR: 27.9386 - ens SSIM: 0.7957\n",
      "Image: 3 - PSNR: 27.2944 - SSIM: 0.7927 - ens PSNR: 27.3784 - ens SSIM: 0.7996\n",
      "Image: 4 - PSNR: 28.7101 - SSIM: 0.7754 - ens PSNR: 28.7815 - ens SSIM: 0.7799\n",
      "Image: 5 - PSNR: 25.5491 - SSIM: 0.6908 - ens PSNR: 25.6345 - ens SSIM: 0.7005\n",
      "Image: 6 - PSNR: 31.0092 - SSIM: 0.8504 - ens PSNR: 31.1241 - ens SSIM: 0.8565\n",
      "Image: 7 - PSNR: 26.6416 - SSIM: 0.7282 - ens PSNR: 26.7198 - ens SSIM: 0.7305\n",
      "Image: 8 - PSNR: 24.0375 - SSIM: 0.6162 - ens PSNR: 24.1322 - ens SSIM: 0.6210\n",
      "Image: 9 - PSNR: 27.1527 - SSIM: 0.7556 - ens PSNR: 27.1815 - ens SSIM: 0.7565\n",
      "Image: 10 - PSNR: 27.9412 - SSIM: 0.7393 - ens PSNR: 28.0187 - ens SSIM: 0.7445\n",
      "Image: 11 - PSNR: 26.2587 - SSIM: 0.7794 - ens PSNR: 26.4139 - ens SSIM: 0.7867\n",
      "Image: 12 - PSNR: 26.6520 - SSIM: 0.6815 - ens PSNR: 26.7146 - ens SSIM: 0.6847\n",
      "Image: 13 - PSNR: 27.9866 - SSIM: 0.7363 - ens PSNR: 28.0343 - ens SSIM: 0.7394\n",
      "Image: 14 - PSNR: 28.5456 - SSIM: 0.7753 - ens PSNR: 28.6531 - ens SSIM: 0.7822\n",
      "Image: 15 - PSNR: 25.1152 - SSIM: 0.6655 - ens PSNR: 25.1658 - ens SSIM: 0.6685\n",
      "Image: 16 - PSNR: 25.6319 - SSIM: 0.7693 - ens PSNR: 25.7452 - ens SSIM: 0.7737\n",
      "Image: 17 - PSNR: 31.2912 - SSIM: 0.8145 - ens PSNR: 31.3714 - ens SSIM: 0.8208\n",
      "Image: 18 - PSNR: 31.0186 - SSIM: 0.8397 - ens PSNR: 31.1534 - ens SSIM: 0.8473\n",
      "Image: 19 - PSNR: 25.8833 - SSIM: 0.6947 - ens PSNR: 25.9226 - ens SSIM: 0.6980\n",
      "Image: 20 - PSNR: 26.2912 - SSIM: 0.7609 - ens PSNR: 26.3689 - ens SSIM: 0.7669\n",
      "Image: 21 - PSNR: 23.8010 - SSIM: 0.7719 - ens PSNR: 23.9321 - ens SSIM: 0.7786\n",
      "Image: 22 - PSNR: 24.9318 - SSIM: 0.6891 - ens PSNR: 25.0233 - ens SSIM: 0.6942\n",
      "Image: 23 - PSNR: 26.5514 - SSIM: 0.7728 - ens PSNR: 26.5485 - ens SSIM: 0.7761\n",
      "Image: 24 - PSNR: 26.9195 - SSIM: 0.7936 - ens PSNR: 27.0205 - ens SSIM: 0.7986\n",
      "Image: 25 - PSNR: 27.2254 - SSIM: 0.8400 - ens PSNR: 27.2977 - ens SSIM: 0.8465\n",
      "Image: 26 - PSNR: 26.6457 - SSIM: 0.6991 - ens PSNR: 26.7088 - ens SSIM: 0.7024\n",
      "Image: 27 - PSNR: 28.1598 - SSIM: 0.7446 - ens PSNR: 28.2215 - ens SSIM: 0.7488\n",
      "Image: 28 - PSNR: 30.2049 - SSIM: 0.8150 - ens PSNR: 30.7766 - ens SSIM: 0.8401\n",
      "Image: 29 - PSNR: 21.5855 - SSIM: 0.6512 - ens PSNR: 21.6182 - ens SSIM: 0.6546\n",
      "Image: 30 - PSNR: 27.9819 - SSIM: 0.7161 - ens PSNR: 28.0604 - ens SSIM: 0.7211\n",
      "Image: 31 - PSNR: 22.1340 - SSIM: 0.6253 - ens PSNR: 22.1859 - ens SSIM: 0.6278\n",
      "Image: 32 - PSNR: 24.8742 - SSIM: 0.6970 - ens PSNR: 24.8962 - ens SSIM: 0.6966\n",
      "Image: 33 - PSNR: 25.4022 - SSIM: 0.8046 - ens PSNR: 25.4252 - ens SSIM: 0.8094\n",
      "Image: 34 - PSNR: 31.6269 - SSIM: 0.8280 - ens PSNR: 31.7989 - ens SSIM: 0.8346\n",
      "Image: 35 - PSNR: 26.2333 - SSIM: 0.7022 - ens PSNR: 26.2789 - ens SSIM: 0.7054\n",
      "Image: 36 - PSNR: 26.0212 - SSIM: 0.3788 - ens PSNR: 26.0440 - ens SSIM: 0.3804\n",
      "Image: 37 - PSNR: 26.8169 - SSIM: 0.6600 - ens PSNR: 26.8715 - ens SSIM: 0.6647\n",
      "Image: 38 - PSNR: 27.1792 - SSIM: 0.6922 - ens PSNR: 27.2480 - ens SSIM: 0.6959\n",
      "Image: 39 - PSNR: 30.6813 - SSIM: 0.8651 - ens PSNR: 30.8697 - ens SSIM: 0.8724\n",
      "Image: 40 - PSNR: 27.2175 - SSIM: 0.7058 - ens PSNR: 27.3020 - ens SSIM: 0.7091\n",
      "Image: 41 - PSNR: 25.6990 - SSIM: 0.7050 - ens PSNR: 25.7473 - ens SSIM: 0.7088\n",
      "Image: 42 - PSNR: 27.1210 - SSIM: 0.7207 - ens PSNR: 27.2105 - ens SSIM: 0.7263\n",
      "Image: 43 - PSNR: 28.2878 - SSIM: 0.8361 - ens PSNR: 28.4249 - ens SSIM: 0.8410\n",
      "Image: 44 - PSNR: 24.5374 - SSIM: 0.6802 - ens PSNR: 24.5858 - ens SSIM: 0.6841\n",
      "Image: 45 - PSNR: 31.5560 - SSIM: 0.7965 - ens PSNR: 31.6759 - ens SSIM: 0.8039\n",
      "Image: 46 - PSNR: 23.9157 - SSIM: 0.6440 - ens PSNR: 24.0117 - ens SSIM: 0.6484\n",
      "Image: 47 - PSNR: 24.6889 - SSIM: 0.6392 - ens PSNR: 24.7016 - ens SSIM: 0.6402\n",
      "Image: 48 - PSNR: 27.1905 - SSIM: 0.8573 - ens PSNR: 27.3914 - ens SSIM: 0.8647\n",
      "Image: 49 - PSNR: 29.6970 - SSIM: 0.7475 - ens PSNR: 29.9895 - ens SSIM: 0.7629\n",
      "Image: 50 - PSNR: 24.8335 - SSIM: 0.6291 - ens PSNR: 24.8917 - ens SSIM: 0.6346\n",
      "Image: 51 - PSNR: 25.6236 - SSIM: 0.7094 - ens PSNR: 25.6851 - ens SSIM: 0.7110\n",
      "Image: 52 - PSNR: 29.8382 - SSIM: 0.7634 - ens PSNR: 30.0610 - ens SSIM: 0.7765\n",
      "Image: 53 - PSNR: 29.6047 - SSIM: 0.6941 - ens PSNR: 29.6617 - ens SSIM: 0.6984\n",
      "Image: 54 - PSNR: 27.0402 - SSIM: 0.7408 - ens PSNR: 27.1258 - ens SSIM: 0.7456\n",
      "Image: 55 - PSNR: 25.4796 - SSIM: 0.6656 - ens PSNR: 25.5566 - ens SSIM: 0.6682\n",
      "Image: 56 - PSNR: 21.7864 - SSIM: 0.5445 - ens PSNR: 21.8350 - ens SSIM: 0.5485\n",
      "Image: 57 - PSNR: 26.2917 - SSIM: 0.6840 - ens PSNR: 26.3302 - ens SSIM: 0.6894\n",
      "Image: 58 - PSNR: 29.4972 - SSIM: 0.7043 - ens PSNR: 29.5420 - ens SSIM: 0.7089\n",
      "Image: 59 - PSNR: 28.8115 - SSIM: 0.7021 - ens PSNR: 28.8718 - ens SSIM: 0.7065\n",
      "Image: 60 - PSNR: 29.1830 - SSIM: 0.7208 - ens PSNR: 29.2532 - ens SSIM: 0.7268\n",
      "Image: 61 - PSNR: 28.0060 - SSIM: 0.7084 - ens PSNR: 28.0673 - ens SSIM: 0.7144\n",
      "Image: 62 - PSNR: 31.7046 - SSIM: 0.9075 - ens PSNR: 31.9622 - ens SSIM: 0.9148\n",
      "Image: 63 - PSNR: 24.5237 - SSIM: 0.7028 - ens PSNR: 24.5600 - ens SSIM: 0.7035\n",
      "Image: 64 - PSNR: 24.9347 - SSIM: 0.5714 - ens PSNR: 24.9862 - ens SSIM: 0.5737\n",
      "Image: 65 - PSNR: 27.2726 - SSIM: 0.7407 - ens PSNR: 27.3136 - ens SSIM: 0.7445\n",
      "Image: 66 - PSNR: 35.0071 - SSIM: 0.9380 - ens PSNR: 35.2398 - ens SSIM: 0.9476\n",
      "Image: 67 - PSNR: 22.4467 - SSIM: 0.7153 - ens PSNR: 22.5290 - ens SSIM: 0.7180\n",
      "Image: 68 - PSNR: 23.4951 - SSIM: 0.7135 - ens PSNR: 23.6080 - ens SSIM: 0.7291\n",
      "sigma = 30 - PSNR: 26.9842 - SSIM: 0.7273 - ens PSNR: 27.0808 - ens SSIM: 0.7329 \n",
      "\n",
      "extension _gray\n",
      "noise level: 50\n",
      "base directory /content/drive/MyDrive/RDUNet-main/Datasets/Test_matfile\n",
      "Generated Noisy Path: /content/drive/MyDrive/RDUNet-main/Datasets/Test_matfile/noisy_bsd68_50_gray.mat\n",
      "Generated GT Path: /content/drive/MyDrive/RDUNet-main/Datasets/Test_matfile/bsd68_label_gray.mat\n",
      "68\n",
      "Image: 1 - PSNR: 21.8260 - SSIM: 0.4363 - ens PSNR: 21.8939 - ens SSIM: 0.4422\n",
      "Image: 2 - PSNR: 24.9311 - SSIM: 0.6370 - ens PSNR: 25.4941 - ens SSIM: 0.7076\n",
      "Image: 3 - PSNR: 24.1694 - SSIM: 0.6809 - ens PSNR: 24.2295 - ens SSIM: 0.6990\n",
      "Image: 4 - PSNR: 26.0873 - SSIM: 0.6592 - ens PSNR: 26.1892 - ens SSIM: 0.6722\n",
      "Image: 5 - PSNR: 23.0640 - SSIM: 0.5393 - ens PSNR: 23.2489 - ens SSIM: 0.5740\n",
      "Image: 6 - PSNR: 27.3790 - SSIM: 0.7567 - ens PSNR: 27.6345 - ens SSIM: 0.7858\n",
      "Image: 7 - PSNR: 23.7005 - SSIM: 0.5810 - ens PSNR: 23.8118 - ens SSIM: 0.5915\n",
      "Image: 8 - PSNR: 21.3710 - SSIM: 0.4282 - ens PSNR: 21.4519 - ens SSIM: 0.4341\n",
      "Image: 9 - PSNR: 24.4762 - SSIM: 0.6377 - ens PSNR: 24.5559 - ens SSIM: 0.6474\n",
      "Image: 10 - PSNR: 25.2879 - SSIM: 0.6167 - ens PSNR: 25.3835 - ens SSIM: 0.6298\n",
      "Image: 11 - PSNR: 23.1594 - SSIM: 0.6338 - ens PSNR: 23.2044 - ens SSIM: 0.6429\n",
      "Image: 12 - PSNR: 23.9869 - SSIM: 0.5258 - ens PSNR: 24.0692 - ens SSIM: 0.5340\n",
      "Image: 13 - PSNR: 25.4701 - SSIM: 0.6049 - ens PSNR: 25.5433 - ens SSIM: 0.6152\n",
      "Image: 14 - PSNR: 25.8161 - SSIM: 0.6671 - ens PSNR: 25.9451 - ens SSIM: 0.6882\n",
      "Image: 15 - PSNR: 22.6726 - SSIM: 0.5235 - ens PSNR: 22.7088 - ens SSIM: 0.5323\n",
      "Image: 16 - PSNR: 22.4288 - SSIM: 0.6319 - ens PSNR: 22.5120 - ens SSIM: 0.6404\n",
      "Image: 17 - PSNR: 28.6085 - SSIM: 0.7202 - ens PSNR: 28.8285 - ens SSIM: 0.7457\n",
      "Image: 18 - PSNR: 28.1564 - SSIM: 0.7476 - ens PSNR: 28.3039 - ens SSIM: 0.7718\n",
      "Image: 19 - PSNR: 23.4081 - SSIM: 0.5871 - ens PSNR: 23.5426 - ens SSIM: 0.6038\n",
      "Image: 20 - PSNR: 23.6714 - SSIM: 0.6618 - ens PSNR: 23.7494 - ens SSIM: 0.6811\n",
      "Image: 21 - PSNR: 20.5817 - SSIM: 0.6019 - ens PSNR: 20.6729 - ens SSIM: 0.6128\n",
      "Image: 22 - PSNR: 22.2115 - SSIM: 0.5350 - ens PSNR: 22.2760 - ens SSIM: 0.5412\n",
      "Image: 23 - PSNR: 23.4969 - SSIM: 0.6374 - ens PSNR: 23.5184 - ens SSIM: 0.6489\n",
      "Image: 24 - PSNR: 24.1985 - SSIM: 0.6860 - ens PSNR: 24.3032 - ens SSIM: 0.6971\n",
      "Image: 25 - PSNR: 24.6313 - SSIM: 0.7329 - ens PSNR: 24.6723 - ens SSIM: 0.7505\n",
      "Image: 26 - PSNR: 24.4392 - SSIM: 0.5934 - ens PSNR: 24.5392 - ens SSIM: 0.6032\n",
      "Image: 27 - PSNR: 25.8427 - SSIM: 0.6388 - ens PSNR: 25.9025 - ens SSIM: 0.6497\n",
      "Image: 28 - PSNR: 27.2088 - SSIM: 0.7116 - ens PSNR: 28.3748 - ens SSIM: 0.7967\n",
      "Image: 29 - PSNR: 18.8519 - SSIM: 0.4442 - ens PSNR: 18.8358 - ens SSIM: 0.4459\n",
      "Image: 30 - PSNR: 25.3482 - SSIM: 0.5806 - ens PSNR: 25.4531 - ens SSIM: 0.5946\n",
      "Image: 31 - PSNR: 19.5485 - SSIM: 0.3632 - ens PSNR: 19.5871 - ens SSIM: 0.3661\n",
      "Image: 32 - PSNR: 22.2141 - SSIM: 0.5084 - ens PSNR: 22.2576 - ens SSIM: 0.5117\n",
      "Image: 33 - PSNR: 22.7874 - SSIM: 0.6655 - ens PSNR: 22.8507 - ens SSIM: 0.6859\n",
      "Image: 34 - PSNR: 28.3668 - SSIM: 0.7550 - ens PSNR: 28.5933 - ens SSIM: 0.7747\n",
      "Image: 35 - PSNR: 23.9460 - SSIM: 0.5862 - ens PSNR: 24.0117 - ens SSIM: 0.5974\n",
      "Image: 36 - PSNR: 24.9019 - SSIM: 0.3392 - ens PSNR: 24.9827 - ens SSIM: 0.3458\n",
      "Image: 37 - PSNR: 24.8289 - SSIM: 0.5602 - ens PSNR: 24.9198 - ens SSIM: 0.5740\n",
      "Image: 38 - PSNR: 24.6542 - SSIM: 0.5376 - ens PSNR: 24.7775 - ens SSIM: 0.5494\n",
      "Image: 39 - PSNR: 26.7575 - SSIM: 0.7545 - ens PSNR: 27.0793 - ens SSIM: 0.7794\n",
      "Image: 40 - PSNR: 24.2681 - SSIM: 0.6004 - ens PSNR: 24.4303 - ens SSIM: 0.6156\n",
      "Image: 41 - PSNR: 23.0959 - SSIM: 0.5757 - ens PSNR: 23.1608 - ens SSIM: 0.5876\n",
      "Image: 42 - PSNR: 24.3886 - SSIM: 0.6221 - ens PSNR: 24.4896 - ens SSIM: 0.6398\n",
      "Image: 43 - PSNR: 24.9016 - SSIM: 0.7351 - ens PSNR: 25.0445 - ens SSIM: 0.7503\n",
      "Image: 44 - PSNR: 22.2969 - SSIM: 0.5180 - ens PSNR: 22.3451 - ens SSIM: 0.5271\n",
      "Image: 45 - PSNR: 28.4294 - SSIM: 0.7033 - ens PSNR: 28.6643 - ens SSIM: 0.7286\n",
      "Image: 46 - PSNR: 21.1443 - SSIM: 0.4889 - ens PSNR: 21.2127 - ens SSIM: 0.4966\n",
      "Image: 47 - PSNR: 22.3928 - SSIM: 0.4697 - ens PSNR: 22.4246 - ens SSIM: 0.4723\n",
      "Image: 48 - PSNR: 23.5771 - SSIM: 0.7422 - ens PSNR: 23.8351 - ens SSIM: 0.7637\n",
      "Image: 49 - PSNR: 27.1511 - SSIM: 0.6586 - ens PSNR: 27.7719 - ens SSIM: 0.7080\n",
      "Image: 50 - PSNR: 22.7171 - SSIM: 0.4889 - ens PSNR: 22.7815 - ens SSIM: 0.5049\n",
      "Image: 51 - PSNR: 22.1546 - SSIM: 0.5818 - ens PSNR: 22.2215 - ens SSIM: 0.5924\n",
      "Image: 52 - PSNR: 26.9400 - SSIM: 0.6596 - ens PSNR: 27.5318 - ens SSIM: 0.7159\n",
      "Image: 53 - PSNR: 27.1658 - SSIM: 0.6010 - ens PSNR: 27.3336 - ens SSIM: 0.6204\n",
      "Image: 54 - PSNR: 24.1398 - SSIM: 0.6202 - ens PSNR: 24.2337 - ens SSIM: 0.6310\n",
      "Image: 55 - PSNR: 22.9523 - SSIM: 0.5294 - ens PSNR: 23.0424 - ens SSIM: 0.5360\n",
      "Image: 56 - PSNR: 19.6277 - SSIM: 0.3227 - ens PSNR: 19.6824 - ens SSIM: 0.3294\n",
      "Image: 57 - PSNR: 24.4703 - SSIM: 0.5666 - ens PSNR: 24.5580 - ens SSIM: 0.5828\n",
      "Image: 58 - PSNR: 27.2923 - SSIM: 0.6111 - ens PSNR: 27.4411 - ens SSIM: 0.6323\n",
      "Image: 59 - PSNR: 26.8418 - SSIM: 0.6226 - ens PSNR: 26.9613 - ens SSIM: 0.6392\n",
      "Image: 60 - PSNR: 27.0410 - SSIM: 0.6304 - ens PSNR: 27.2105 - ens SSIM: 0.6530\n",
      "Image: 61 - PSNR: 25.7736 - SSIM: 0.6233 - ens PSNR: 25.8965 - ens SSIM: 0.6421\n",
      "Image: 62 - PSNR: 27.8566 - SSIM: 0.8345 - ens PSNR: 28.0421 - ens SSIM: 0.8555\n",
      "Image: 63 - PSNR: 22.0613 - SSIM: 0.5358 - ens PSNR: 22.1024 - ens SSIM: 0.5384\n",
      "Image: 64 - PSNR: 23.0120 - SSIM: 0.4452 - ens PSNR: 23.0813 - ens SSIM: 0.4506\n",
      "Image: 65 - PSNR: 24.7499 - SSIM: 0.6180 - ens PSNR: 24.8337 - ens SSIM: 0.6343\n",
      "Image: 66 - PSNR: 30.2560 - SSIM: 0.8417 - ens PSNR: 30.5723 - ens SSIM: 0.8791\n",
      "Image: 67 - PSNR: 19.8100 - SSIM: 0.5375 - ens PSNR: 19.8323 - ens SSIM: 0.5376\n",
      "Image: 68 - PSNR: 20.9375 - SSIM: 0.5207 - ens PSNR: 21.1778 - ens SSIM: 0.5780\n",
      "sigma = 50 - PSNR: 24.2931 - SSIM: 0.6002 - ens PSNR: 24.4386 - ens SSIM: 0.6177 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#main_test.py\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "#from model import RDUNet\n",
    "# from model import RatUNet, BasicBlock\n",
    "from torchvision.transforms import transforms\n",
    "from skimage import io\n",
    "from skimage.metrics import structural_similarity, peak_signal_noise_ratio\n",
    "\n",
    "# from utils import build_ensemble, separate_ensemble, predict_ensemble, mod_pad, mod_crop\n",
    "\n",
    "f1=open('/content/drive/MyDrive/RDUNet-main/Results/test_gray_bsd.txt','w')\n",
    "\n",
    "def predict(model, noisy_dataset, gt_dataset, device, padding, n_channels, results_path):\n",
    "    # Load test datasets in format .mat\n",
    "    X = sio.loadmat(noisy_dataset)['data'].flatten()\n",
    "    Y = sio.loadmat(gt_dataset)['label'].flatten()\n",
    "\n",
    "\n",
    "    y_pred, y_pred_ens = [], []\n",
    "    psnr_list, ssim_list = [], []\n",
    "    ens_psnr_list, ens_ssim_list = [], []\n",
    "\n",
    "    n_images = len(X)\n",
    "    print(n_images)\n",
    "    multi_channel = True if n_channels == 3 else False\n",
    "\n",
    "    for i in range(n_images):\n",
    "        x, y = X[i], Y[i]\n",
    "        # print(\"image_x\",x.shape)\n",
    "        # print('image_y',y.shape)\n",
    "\n",
    "        if padding:\n",
    "            x, size = mod_pad(x, 8)\n",
    "            # print(x.shape)\n",
    "            # print(size)\n",
    "        else:\n",
    "            x, y = mod_crop(x, 8), mod_crop(y, 8)\n",
    "\n",
    "\n",
    "        x = build_ensemble(x, normalize=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_hat_ens = predict_ensemble(model, x, device)\n",
    "            # print(y_hat_ens.shape)\n",
    "            y_hat_ens, y_hat = separate_ensemble(y_hat_ens, return_single=True)\n",
    "\n",
    "            if padding:\n",
    "                y_hat = y_hat[:size[0], :size[1], ...]\n",
    "                y_hat_ens = y_hat_ens[:size[0], :size[1], ...]\n",
    "\n",
    "            y_pred.append(y_hat)\n",
    "            y_pred_ens.append(y_hat_ens)\n",
    "            psnr = peak_signal_noise_ratio(y, y_hat, data_range=1.)\n",
    "            ssim = structural_similarity(y, y_hat, data_range=1., multichannel=multi_channel, gaussian_weights=True,\n",
    "                                         sigma=1.5, use_sample_covariance=False)\n",
    "\n",
    "            psnr_ens = peak_signal_noise_ratio(y, y_hat_ens, data_range=1.)\n",
    "            ssim_ens = structural_similarity(y, y_hat_ens, data_range=1., multichannel=multi_channel,\n",
    "                                             gaussian_weights=True, sigma=1.5, use_sample_covariance=False)\n",
    "\n",
    "            psnr_list.append(psnr)\n",
    "            ssim_list.append(ssim)\n",
    "\n",
    "            ens_psnr_list.append(psnr_ens)\n",
    "            ens_ssim_list.append(ssim_ens)\n",
    "            print('Image: {} - PSNR: {:.4f} - SSIM: {:.4f} - ens PSNR: {:.4f}'\n",
    "                  ' - ens SSIM: {:.4f}'.format(i + 1, psnr, ssim, psnr_ens, ssim_ens))\n",
    "            f1.write('Image: {} - PSNR: {:.4f} - SSIM: {:.4f} - ens PSNR: {:.4f}'\n",
    "                  ' - ens SSIM: {:.4f} \\n'.format(i + 1, psnr, ssim, psnr_ens, ssim_ens))\n",
    "\n",
    "\n",
    "    if results_path is not None:\n",
    "        for i in range(n_images):\n",
    "            y_hat = (255 * y_pred[i]).astype('uint8')\n",
    "            y_hat_ens = (255 * y_pred_ens[i]).astype('uint8')\n",
    "\n",
    "            y_hat = np.squeeze(y_hat)\n",
    "            y_hat_ens = np.squeeze(y_hat_ens)\n",
    "\n",
    "            os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "            name = os.path.join(results_path, '{}_{:.4f}_{:.4f}.png'.format(i, psnr_list[i], ssim_list[i]))\n",
    "            io.imsave(name, y_hat)\n",
    "\n",
    "            name = os.path.join(results_path, '{}_{:.4f}_{:.4f}_ens.png'.format(i, ens_psnr_list[i], ens_ssim_list[i]))\n",
    "            io.imsave(name, y_hat_ens)\n",
    "\n",
    "    return np.mean(psnr_list), np.mean(ssim_list), np.mean(ens_psnr_list), np.mean(ens_ssim_list)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open('/content/drive/MyDrive/RDUNet-main/config.yaml', 'r') as stream:\n",
    "        config = yaml.safe_load(stream)\n",
    "\n",
    "    model_params = config['model']\n",
    "    test_params = config['test']\n",
    "    n_channels = model_params['channels']\n",
    "\n",
    "    if n_channels == 3:\n",
    "        model_path = join(test_params['pretrained models path'], 'model_color.pth')\n",
    "        noisy_datasets = ['noisy_McMaster_']  # Also tested in Kodak24 and Urban100 datasets.\n",
    "        gt_datasets = ['McMaster_label']\n",
    "\n",
    "    else:\n",
    "        model_path = join(test_params['pretrained models path'], 'model_gray.pth')\n",
    "        noisy_datasets = ['noisy_bsd68_']   # Also tested in BSD68 and Kodak24 datasets.\n",
    "        gt_datasets = ['bsd68_label']\n",
    "\n",
    "    model_params = config['model']\n",
    "    model = DeepRDU(**model_params)\n",
    "    # model = DN(**model_params)\n",
    "    # model = RDUNet(**model_params)\n",
    "    # model = RatUNet(BasicBlock, 64)\n",
    "\n",
    "    device = torch.device(test_params['device'])\n",
    "    print(\"Using device: {}\".format(device))\n",
    "\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    base_directory = \"/content/drive/MyDrive/RDUNet-main/Datasets/Test_matfile\"\n",
    "\n",
    "    for noisy_dataset, gt_dataset in zip(noisy_datasets, gt_datasets):\n",
    "        print('Noisy Dataset: ', noisy_dataset)\n",
    "        print(\"Gt Dataset:\", gt_dataset)\n",
    "\n",
    "        for noise_level in test_params['noise levels']:\n",
    "            extension = '_color' if model_params['channels'] == 3 else '_gray'\n",
    "            print('extension', extension)\n",
    "            print('noise level:', noise_level)\n",
    "            print(\"base directory\", base_directory)\n",
    "\n",
    "            # noisy_path = os.path.join(test_params['/content/drive/MyDrive/RDUNet-main/Datasets/Test_matfile/'], ''.join([noisy_dataset, str(noise_level), extension, '.mat']))\n",
    "            # label_path = join(test_params['/content/drive/MyDrive/RDUNet-main/Datasets/Test_matfile/'], ''.join([gt_dataset, extension, '.mat']))\n",
    "            noisy_path = os.path.join(base_directory, ''.join([noisy_dataset, str(noise_level), extension, '.mat']))\n",
    "            print(\"Generated Noisy Path:\", noisy_path)\n",
    "            label_path = os.path.join(base_directory, ''.join([gt_dataset, extension, '.mat']))\n",
    "            print(\"Generated GT Path:\", label_path)\n",
    "\n",
    "            if test_params['save images']:\n",
    "                save_path = join(\n",
    "                    test_params['results path'], ''.join([noisy_dataset.replace('noisy_', ''), 'sigma_', str(noise_level)])\n",
    "                )\n",
    "            else:\n",
    "                save_path = None\n",
    "\n",
    "            psnr, ssim, psnr_ens, ssim_ens = predict(model, noisy_path, label_path, device,\n",
    "                                                     test_params['padding'],  n_channels, save_path)\n",
    "\n",
    "            message = 'sigma = {} - PSNR: {:.4f} - SSIM: {:.4f} - ens PSNR: {:.4f} - ens SSIM: {:.4f} \\n'\n",
    "            print(message.format(noise_level, np.around(psnr, decimals=4), np.around(ssim, decimals=4),\n",
    "                                 np.around(psnr_ens, decimals=4), np.around(ssim_ens, decimals=4)))\n",
    "            f1.write(message.format(noise_level, np.around(psnr, decimals=4), np.around(ssim, decimals=4),\n",
    "                                 np.around(psnr_ens, decimals=4), np.around(ssim_ens, decimals=4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QpOdnbE2VN0x",
    "outputId": "a83a84fa-8f87-46f2-abe5-2037ceb900da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]           1,792\n",
      "            Conv2d-2           [-1, 64, 64, 64]          36,928\n",
      "            Conv2d-3           [-1, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-4           [-1, 64, 64, 64]             128\n",
      "             PReLU-5           [-1, 64, 64, 64]              64\n",
      "            Conv2d-6           [-1, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-7           [-1, 64, 64, 64]             128\n",
      "             PReLU-8           [-1, 64, 64, 64]              64\n",
      "          ResBlock-9           [-1, 64, 64, 64]               0\n",
      "            PReLU-10           [-1, 64, 64, 64]              64\n",
      "     EncoderBlock-11           [-1, 64, 64, 64]               0\n",
      "        MaxPool2d-12           [-1, 64, 32, 32]               0\n",
      "           Conv2d-13          [-1, 128, 32, 32]          73,856\n",
      "           Conv2d-14          [-1, 128, 32, 32]         147,584\n",
      "           Conv2d-15          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-16          [-1, 128, 32, 32]             256\n",
      "            PReLU-17          [-1, 128, 32, 32]             128\n",
      "           Conv2d-18          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-19          [-1, 128, 32, 32]             256\n",
      "            PReLU-20          [-1, 128, 32, 32]             128\n",
      "         ResBlock-21          [-1, 128, 32, 32]               0\n",
      "            PReLU-22          [-1, 128, 32, 32]             128\n",
      "     EncoderBlock-23          [-1, 128, 32, 32]               0\n",
      "          Down_DC-24          [-1, 128, 32, 32]               0\n",
      "           Conv2d-25          [-1, 128, 32, 32]         147,584\n",
      "           Conv2d-26          [-1, 128, 32, 32]         147,584\n",
      "           Conv2d-27          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-28          [-1, 128, 32, 32]             256\n",
      "            PReLU-29          [-1, 128, 32, 32]             128\n",
      "           Conv2d-30          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-31          [-1, 128, 32, 32]             256\n",
      "            PReLU-32          [-1, 128, 32, 32]             128\n",
      "         ResBlock-33          [-1, 128, 32, 32]               0\n",
      "            PReLU-34          [-1, 128, 32, 32]             128\n",
      "     EncoderBlock-35          [-1, 128, 32, 32]               0\n",
      "        MaxPool2d-36          [-1, 128, 16, 16]               0\n",
      "           Conv2d-37          [-1, 256, 16, 16]         295,168\n",
      "           Conv2d-38          [-1, 256, 16, 16]         590,080\n",
      "           Conv2d-39          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-40          [-1, 256, 16, 16]             512\n",
      "            PReLU-41          [-1, 256, 16, 16]             256\n",
      "           Conv2d-42          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-43          [-1, 256, 16, 16]             512\n",
      "            PReLU-44          [-1, 256, 16, 16]             256\n",
      "         ResBlock-45          [-1, 256, 16, 16]               0\n",
      "            PReLU-46          [-1, 256, 16, 16]             256\n",
      "     EncoderBlock-47          [-1, 256, 16, 16]               0\n",
      "          Down_DC-48          [-1, 256, 16, 16]               0\n",
      "           Conv2d-49          [-1, 256, 16, 16]         590,080\n",
      "           Conv2d-50          [-1, 256, 16, 16]         590,080\n",
      "           Conv2d-51          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-52          [-1, 256, 16, 16]             512\n",
      "            PReLU-53          [-1, 256, 16, 16]             256\n",
      "           Conv2d-54          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-55          [-1, 256, 16, 16]             512\n",
      "            PReLU-56          [-1, 256, 16, 16]             256\n",
      "         ResBlock-57          [-1, 256, 16, 16]               0\n",
      "            PReLU-58          [-1, 256, 16, 16]             256\n",
      "     EncoderBlock-59          [-1, 256, 16, 16]               0\n",
      "        MaxPool2d-60            [-1, 256, 8, 8]               0\n",
      "           Conv2d-61            [-1, 512, 8, 8]       1,180,160\n",
      "           Conv2d-62            [-1, 512, 8, 8]       2,359,808\n",
      "           Conv2d-63            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 8, 8]           1,024\n",
      "            PReLU-65            [-1, 512, 8, 8]             512\n",
      "           Conv2d-66            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-67            [-1, 512, 8, 8]           1,024\n",
      "            PReLU-68            [-1, 512, 8, 8]             512\n",
      "         ResBlock-69            [-1, 512, 8, 8]               0\n",
      "            PReLU-70            [-1, 512, 8, 8]             512\n",
      "     EncoderBlock-71            [-1, 512, 8, 8]               0\n",
      "          Down_DC-72            [-1, 512, 8, 8]               0\n",
      "           Conv2d-73            [-1, 512, 8, 8]       2,359,808\n",
      "           Conv2d-74            [-1, 512, 8, 8]       2,359,808\n",
      "           Conv2d-75            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-76            [-1, 512, 8, 8]           1,024\n",
      "            PReLU-77            [-1, 512, 8, 8]             512\n",
      "           Conv2d-78            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-79            [-1, 512, 8, 8]           1,024\n",
      "            PReLU-80            [-1, 512, 8, 8]             512\n",
      "         ResBlock-81            [-1, 512, 8, 8]               0\n",
      "            PReLU-82            [-1, 512, 8, 8]             512\n",
      "     EncoderBlock-83            [-1, 512, 8, 8]               0\n",
      "        MaxPool2d-84            [-1, 512, 4, 4]               0\n",
      "           Conv2d-85           [-1, 1024, 4, 4]       4,719,616\n",
      "           Conv2d-86           [-1, 1024, 4, 4]       9,438,208\n",
      "           Conv2d-87           [-1, 1024, 4, 4]       9,437,184\n",
      "      BatchNorm2d-88           [-1, 1024, 4, 4]           2,048\n",
      "            PReLU-89           [-1, 1024, 4, 4]           1,024\n",
      "           Conv2d-90           [-1, 1024, 4, 4]       9,437,184\n",
      "      BatchNorm2d-91           [-1, 1024, 4, 4]           2,048\n",
      "            PReLU-92           [-1, 1024, 4, 4]           1,024\n",
      "         ResBlock-93           [-1, 1024, 4, 4]               0\n",
      "            PReLU-94           [-1, 1024, 4, 4]           1,024\n",
      "     EncoderBlock-95           [-1, 1024, 4, 4]               0\n",
      "          Down_DC-96           [-1, 1024, 4, 4]               0\n",
      "           Conv2d-97           [-1, 1024, 4, 4]       9,438,208\n",
      "           Conv2d-98           [-1, 1024, 4, 4]       9,438,208\n",
      "           Conv2d-99           [-1, 1024, 4, 4]       9,437,184\n",
      "     BatchNorm2d-100           [-1, 1024, 4, 4]           2,048\n",
      "           PReLU-101           [-1, 1024, 4, 4]           1,024\n",
      "          Conv2d-102           [-1, 1024, 4, 4]       9,437,184\n",
      "     BatchNorm2d-103           [-1, 1024, 4, 4]           2,048\n",
      "           PReLU-104           [-1, 1024, 4, 4]           1,024\n",
      "        ResBlock-105           [-1, 1024, 4, 4]               0\n",
      "           PReLU-106           [-1, 1024, 4, 4]           1,024\n",
      " EnDecoderModule-107           [-1, 1024, 4, 4]               0\n",
      " ConvTranspose2d-108           [-1, 1024, 8, 8]       4,195,328\n",
      "         UpBlock-109           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-110            [-1, 512, 8, 8]       4,719,104\n",
      "          Conv2d-111            [-1, 512, 8, 8]       2,359,808\n",
      "          Conv2d-112            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-113            [-1, 512, 8, 8]           1,024\n",
      "           PReLU-114            [-1, 512, 8, 8]             512\n",
      "          Conv2d-115            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-116            [-1, 512, 8, 8]           1,024\n",
      "           PReLU-117            [-1, 512, 8, 8]             512\n",
      "        ResBlock-118            [-1, 512, 8, 8]               0\n",
      "           PReLU-119            [-1, 512, 8, 8]             512\n",
      "    DecoderBlock-120            [-1, 512, 8, 8]               0\n",
      " ConvTranspose2d-121          [-1, 512, 16, 16]       2,097,664\n",
      "         UpBlock-122          [-1, 512, 16, 16]               0\n",
      "          Conv2d-123          [-1, 256, 16, 16]       1,179,904\n",
      "          Conv2d-124          [-1, 256, 16, 16]         590,080\n",
      "          Conv2d-125          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-126          [-1, 256, 16, 16]             512\n",
      "           PReLU-127          [-1, 256, 16, 16]             256\n",
      "          Conv2d-128          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-129          [-1, 256, 16, 16]             512\n",
      "           PReLU-130          [-1, 256, 16, 16]             256\n",
      "        ResBlock-131          [-1, 256, 16, 16]               0\n",
      "           PReLU-132          [-1, 256, 16, 16]             256\n",
      "    DecoderBlock-133          [-1, 256, 16, 16]               0\n",
      " ConvTranspose2d-134          [-1, 256, 32, 32]         524,544\n",
      "         UpBlock-135          [-1, 256, 32, 32]               0\n",
      "          Conv2d-136          [-1, 128, 32, 32]         147,584\n",
      "          Conv2d-137          [-1, 128, 32, 32]          16,512\n",
      "          Conv2d-138          [-1, 128, 32, 32]         147,584\n",
      "          Conv2d-139          [-1, 128, 32, 32]          16,512\n",
      "          Conv2d-140          [-1, 128, 32, 32]         147,584\n",
      "          Conv2d-141          [-1, 128, 32, 32]          16,512\n",
      "          Conv2d-142          [-1, 128, 32, 32]         147,584\n",
      "          Conv2d-143          [-1, 128, 32, 32]          16,512\n",
      "        SkipPath-144          [-1, 128, 32, 32]               0\n",
      "          Conv2d-145          [-1, 128, 32, 32]         295,040\n",
      "          Conv2d-146          [-1, 128, 32, 32]         147,584\n",
      "          Conv2d-147          [-1, 128, 32, 32]         147,456\n",
      "     BatchNorm2d-148          [-1, 128, 32, 32]             256\n",
      "           PReLU-149          [-1, 128, 32, 32]             128\n",
      "          Conv2d-150          [-1, 128, 32, 32]         147,456\n",
      "     BatchNorm2d-151          [-1, 128, 32, 32]             256\n",
      "           PReLU-152          [-1, 128, 32, 32]             128\n",
      "        ResBlock-153          [-1, 128, 32, 32]               0\n",
      "           PReLU-154          [-1, 128, 32, 32]             128\n",
      "    DecoderBlock-155          [-1, 128, 32, 32]               0\n",
      " ConvTranspose2d-156          [-1, 128, 64, 64]         131,200\n",
      "         UpBlock-157          [-1, 128, 64, 64]               0\n",
      "          Conv2d-158           [-1, 64, 64, 64]          36,928\n",
      "          Conv2d-159           [-1, 64, 64, 64]           4,160\n",
      "          Conv2d-160           [-1, 64, 64, 64]          36,928\n",
      "          Conv2d-161           [-1, 64, 64, 64]           4,160\n",
      "          Conv2d-162           [-1, 64, 64, 64]          36,928\n",
      "          Conv2d-163           [-1, 64, 64, 64]           4,160\n",
      "          Conv2d-164           [-1, 64, 64, 64]          36,928\n",
      "          Conv2d-165           [-1, 64, 64, 64]           4,160\n",
      "        SkipPath-166           [-1, 64, 64, 64]               0\n",
      "          Conv2d-167           [-1, 64, 64, 64]          73,792\n",
      "          Conv2d-168           [-1, 64, 64, 64]          36,928\n",
      "          Conv2d-169           [-1, 64, 64, 64]          36,864\n",
      "     BatchNorm2d-170           [-1, 64, 64, 64]             128\n",
      "           PReLU-171           [-1, 64, 64, 64]              64\n",
      "          Conv2d-172           [-1, 64, 64, 64]          36,864\n",
      "     BatchNorm2d-173           [-1, 64, 64, 64]             128\n",
      "           PReLU-174           [-1, 64, 64, 64]              64\n",
      "        ResBlock-175           [-1, 64, 64, 64]               0\n",
      "           PReLU-176           [-1, 64, 64, 64]              64\n",
      "    DecoderBlock-177           [-1, 64, 64, 64]               0\n",
      "          Conv2d-178          [-1, 128, 64, 64]         221,312\n",
      "           PReLU-179          [-1, 128, 64, 64]             128\n",
      "          Conv2d-180          [-1, 128, 64, 64]         147,584\n",
      "           PReLU-181          [-1, 128, 64, 64]             128\n",
      "          Conv2d-182          [-1, 128, 64, 64]         147,584\n",
      "           PReLU-183          [-1, 128, 64, 64]             128\n",
      "          Conv2d-184          [-1, 128, 64, 64]         147,584\n",
      "           PReLU-185          [-1, 128, 64, 64]             128\n",
      "          Conv2d-186          [-1, 128, 64, 64]         147,584\n",
      "           PReLU-187          [-1, 128, 64, 64]             128\n",
      "       ConvBlock-188          [-1, 128, 64, 64]               0\n",
      "          Conv2d-189           [-1, 64, 64, 64]           8,256\n",
      "          Conv2d-190            [-1, 1, 64, 64]             129\n",
      "         Softmax-191              [-1, 4096, 1]               0\n",
      "          Conv2d-192            [-1, 128, 1, 1]           8,320\n",
      "         Sigmoid-193               [-1, 1, 128]               0\n",
      "          Conv2d-194           [-1, 64, 64, 64]           8,256\n",
      "          Conv2d-195           [-1, 64, 64, 64]           8,256\n",
      "AdaptiveAvgPool2d-196             [-1, 64, 1, 1]               0\n",
      "         Softmax-197                [-1, 1, 64]               0\n",
      "         Sigmoid-198            [-1, 1, 64, 64]               0\n",
      "          Conv2d-199           [-1, 64, 64, 64]           8,256\n",
      "          Conv2d-200            [-1, 1, 64, 64]             129\n",
      "         Softmax-201              [-1, 4096, 1]               0\n",
      "          Conv2d-202            [-1, 128, 1, 1]           8,320\n",
      "         Sigmoid-203               [-1, 1, 128]               0\n",
      "          Conv2d-204           [-1, 64, 64, 64]           8,256\n",
      "          Conv2d-205           [-1, 64, 64, 64]           8,256\n",
      "AdaptiveAvgPool2d-206             [-1, 64, 1, 1]               0\n",
      "         Softmax-207                [-1, 1, 64]               0\n",
      "         Sigmoid-208            [-1, 1, 64, 64]               0\n",
      "          Conv2d-209           [-1, 64, 64, 64]           8,256\n",
      "          Conv2d-210            [-1, 1, 64, 64]             129\n",
      "         Softmax-211              [-1, 4096, 1]               0\n",
      "          Conv2d-212            [-1, 128, 1, 1]           8,320\n",
      "         Sigmoid-213               [-1, 1, 128]               0\n",
      "          Conv2d-214           [-1, 64, 64, 64]           8,256\n",
      "          Conv2d-215           [-1, 64, 64, 64]           8,256\n",
      "AdaptiveAvgPool2d-216             [-1, 64, 1, 1]               0\n",
      "         Softmax-217                [-1, 1, 64]               0\n",
      "         Sigmoid-218            [-1, 1, 64, 64]               0\n",
      "          Conv2d-219           [-1, 64, 64, 64]           8,256\n",
      "          Conv2d-220            [-1, 1, 64, 64]             129\n",
      "         Softmax-221              [-1, 4096, 1]               0\n",
      "          Conv2d-222            [-1, 128, 1, 1]           8,320\n",
      "         Sigmoid-223               [-1, 1, 128]               0\n",
      "          Conv2d-224           [-1, 64, 64, 64]           8,256\n",
      "          Conv2d-225           [-1, 64, 64, 64]           8,256\n",
      "AdaptiveAvgPool2d-226             [-1, 64, 1, 1]               0\n",
      "         Softmax-227                [-1, 1, 64]               0\n",
      "         Sigmoid-228            [-1, 1, 64, 64]               0\n",
      "          Conv2d-229          [-1, 128, 64, 64]          65,664\n",
      "SequentialPolarizedSelfAttention-230          [-1, 128, 64, 64]               0\n",
      "          Conv2d-231          [-1, 128, 64, 64]         147,584\n",
      "           PReLU-232          [-1, 128, 64, 64]             128\n",
      "          Conv2d-233            [-1, 3, 64, 64]           3,459\n",
      "           PReLU-234            [-1, 3, 64, 64]               3\n",
      "     OutputBlock-235            [-1, 3, 64, 64]               0\n",
      "================================================================\n",
      "Total params: 118,757,962\n",
      "Trainable params: 118,757,962\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 233.98\n",
      "Params size (MB): 453.03\n",
      "Estimated Total Size (MB): 687.05\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import yaml\n",
    "from torchsummary import summary\n",
    "# from your_model_module import DeepRDU  # Replace 'your_model_module' with the actual module where you defined the DeepRDU class\n",
    "\n",
    "# Load the model configuration from the YAML file\n",
    "with open('/content/drive/MyDrive/RDUNet-main/config.yaml', 'r') as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "    model_params = config['model']\n",
    "\n",
    "# Create an instance of the DeepRDU model\n",
    "model = DeepRDU(**model_params)\n",
    "\n",
    "# Move the model to the device (cuda if available, else cpu)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Specify the input size (channels, height, width)\n",
    "input_size = (3, 64, 64)  # Assuming your input size is (3, 256, 256)\n",
    "\n",
    "# Print the model summary\n",
    "summary(model, input_size=input_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WqFTQ0mxPCl"
   },
   "outputs": [],
   "source": [
    "! pip install -q torchview\n",
    "! pip install -q -U graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vLTb7__6xRFP",
    "outputId": "e2b9b612-7943-4d0e-a6ad-5936cb391e5c"
   },
   "outputs": [],
   "source": [
    "from torchview import draw_graph\n",
    "# from torchvision.models import resnet18, GoogLeNet, densenet, vit_b_16\n",
    "import graphviz\n",
    "\n",
    "# when running on VSCode run the below command\n",
    "# svg format on vscode does not give desired result\n",
    "graphviz.set_jupyter_format('png')\n",
    "\n",
    "# model_graph1 = draw_graph(model, input_size=(1,3,64,64), expand_nested=True)\n",
    "model_graph1 = draw_graph(model, input_size=(1,3,64,64), roll=True, expand_nested=True)\n",
    "\n",
    "model_graph1.visual_graph"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hWHFXxNLxMB9",
    "nDlCF-s3gTtI",
    "fiRnv2D0wuvo",
    "vzuRuJ56WFX2",
    "GMCEG74meSud",
    "ms63aB8L9cFU",
    "JcL46JsOhRMJ",
    "UDyAm13PHtl0",
    "mGRndc0WCi1_"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
